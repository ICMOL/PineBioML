<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>PineBioML.selection.classification API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>PineBioML.selection.classification</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from . import SelectionPipeline

import pandas as pd
import numpy as np
from tqdm import tqdm

from joblib import parallel_backend

from sklearn.utils.class_weight import compute_sample_weight

from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression, Lasso
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import LinearSVC
import xgboost as xgb
import lightgbm as lgbm


class Lasso_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using Lasso (L1 penalty) regression as scoring method.  More specifically, L1 penalty will force feature weights to be zeros. 
    As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.

    Lasso_selection will use grid search to find out when all weights vanish.

    Lasso_selection is scale sensitive in numerical and in result.

    &#34;&#34;&#34;

    def __init__(self, k, unbalanced=True, objective=&#34;Regression&#34;):
        &#34;&#34;&#34;
        Args:
            unbalanced (bool, optional): False to imply class weight to samples. Defaults to True.
            objective (str, optional): one of {&#34;Regression&#34;, &#34;BinaryClassification&#34;}
        &#34;&#34;&#34;
        super().__init__(k=k)

        # parameters
        self.objective = objective
        if self.objective in [&#34;regression&#34;, &#34;Regression&#34;]:
            self.regression = True
        else:
            self.regression = False
        self.da = 0.025  # d alpha
        self.upper_init = 50
        self.unbalanced = unbalanced
        self.name = &#34;LassoLinear&#34;

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34;
        refer[
            &#34; publication 1&#34;] = &#34;https://projecteuclid.org/journals/annals-of-statistics/volume-37/issue-5A/High-dimensional-variable-selection/10.1214/08-AOS646.full&#34;
        refer[
            &#34; publication 2&#34;] = &#34;https://www.tandfonline.com/doi/abs/10.1198/016214506000000735?casa_token=5HDhtyCfh40AAAAA:4NxSU97CZubZVpReaQNsSBpqA10_xNhspTQobPnb_z2YXe3Wf-HBHV8OygbqUkmJQPt2Jmp7ZlJPWd0&#34;
        return refer

    def create_kernel(self, C):
        &#34;&#34;&#34;
        Create diffirent kernel according to opjective.

        Args:
            C (float): The coefficient to L1 penalty.

        Returns:
            sklearn.linearmodel: a kernel of sklearn linearmodel
        &#34;&#34;&#34;
        if self.regression:
            return Lasso(alpha=C)
        else:
            return LogisticRegression(penalty=&#34;l1&#34;,
                                      C=1 / C,
                                      solver=&#34;liblinear&#34;,
                                      random_state=142)

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using Lasso (L1 penalty) regression as scoring method.  More specifically, L1 penalty will force feature weights to be zeros. 
        As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.

        Lasso_selection will use grid search to find out when all weights vanish.

         Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        
        To do:
            kfold validation performance threshold.
        
        &#34;&#34;&#34;

        X_train = x.copy()
        y_train = y.copy()

        if self.unbalanced:
            weights = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y_train)
        else:
            weights = np.ones_like(y_train)

        lassoes = []
        # grid searching
        if self.regression:
            grids = np.arange(self.da, self.upper_init, self.da)
        else:
            grids = np.arange(self.upper_init, self.da, -self.da)

        for alpha in tqdm(grids):
            lassoes.append(self.create_kernel(C=alpha))
            lassoes[-1].fit(X_train, y_train, weights)
            alive = (lassoes[-1].coef_ != 0).sum()

            if alive &lt; 1:
                print(&#34;all coefficient are dead, terminated.&#34;)
                break

        coef = np.array([clr.coef_ for clr in lassoes]).flatten()

        self.scores = pd.Series(np.logical_not(coef == 0).sum(axis=0) *
                                self.da,
                                index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()


class Lasso_bisection_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using Lasso (L1 penalty) regression as scoring method.  More specifically, L1 penalty will force feature weights to be zeros. 
    As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.

    Lasso_bisection_selection will use binary search to find out when all weights vanish.
    
    The trace of weight vanishment is not support.

    &#34;&#34;&#34;

    def __init__(self, k, unbalanced=True, objective=&#34;regression&#34;):
        &#34;&#34;&#34;
        Args:
            unbalanced (bool, optional): False to imply class weight to samples. Defaults to True.
            objective (str, optional): one of {&#34;Regression&#34;, &#34;BinaryClassification&#34;}
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.upper_init = 1e+5
        self.lower_init = 1e-5
        self.objective = objective
        if self.objective in [&#34;regression&#34;, &#34;Regression&#34;]:
            self.regression = True
        else:
            self.regression = False
        self.unbalanced = unbalanced
        self.name = &#34;LassoLinear&#34;

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34;
        refer[
            &#34; publication 1&#34;] = &#34;https://projecteuclid.org/journals/annals-of-statistics/volume-37/issue-5A/High-dimensional-variable-selection/10.1214/08-AOS646.full&#34;
        refer[
            &#34; publication 2&#34;] = &#34;https://www.tandfonline.com/doi/abs/10.1198/016214506000000735?casa_token=5HDhtyCfh40AAAAA:4NxSU97CZubZVpReaQNsSBpqA10_xNhspTQobPnb_z2YXe3Wf-HBHV8OygbqUkmJQPt2Jmp7ZlJPWd0&#34;
        return refer

    def create_kernel(self, C):
        if self.regression:
            return Lasso(alpha=C)
        else:
            return LogisticRegression(penalty=&#34;l1&#34;,
                                      C=1 / C,
                                      solver=&#34;liblinear&#34;,
                                      random_state=142)

    def Select(self, x, y):
        &#34;&#34;&#34;
        Using Lasso (L1 penalty) regression as scoring method.  More specifically, L1 penalty will force feature weights to be zeros. 
        As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.

        Lasso_bisection_selection will use binary search to find out when all weights vanish.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.
            k (int): Number of feature to select. The result may less than k

        Returns:
            pandas.Series: The score for k selected features. May less than k.
        &#34;&#34;&#34;

        # train test split
        X_train = x.copy()
        y_train = y.copy()

        if self.unbalanced:
            weights = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y_train)
        else:
            weights = np.ones_like(y_train)

        if self.k == -1:
            self.k = x.shape[0]

        y_train = OneHotEncoder(sparse_output=False).fit_transform(
            y_train.to_numpy().reshape(-1, 1))

        lassoes = []
        # Bisection searching
        ### standardize x
        X_train = X_train / X_train.values.std()

        upper = self.upper_init
        lassoes.append(self.create_kernel(C=upper))
        lassoes[-1].fit(X_train, y_train, weights)
        upper_alive = (self.coef_to_importance(lassoes[-1].coef_) != 0).sum()
        #print(upper, upper_alive)

        lower = self.lower_init
        lassoes.append(self.create_kernel(C=lower))
        lassoes[-1].fit(X_train, y_train, weights)
        lower_alive = (self.coef_to_importance(lassoes[-1].coef_) != 0).sum()
        #print(lower, lower_alive)

        counter = 0
        while not lower_alive == self.k:
            alpha = (upper + lower) / 2
            lassoes.append(self.create_kernel(C=alpha))
            lassoes[-1].fit(X_train, y_train, weights)
            alive = (self.coef_to_importance(lassoes[-1].coef_) != 0).sum()
            #print(alive, alpha)

            if alive &gt;= self.k:
                lower = alpha
                lower_alive = alive
            else:
                upper = alpha
                upper_alive = alive

            counter += 1
            if counter &gt; 40:
                break

        coef = np.array([clr.coef_ for clr in lassoes])

        self.scores = pd.Series(self.coef_to_importance(coef[-1]),
                                index=x.columns,
                                name=self.name).sort_values(ascending=False)
        self.selected_score = self.scores.head(self.k)
        return self.selected_score

    def coef_to_importance(self, coef):
        return np.linalg.norm(coef, ord=2, axis=0)


class multi_Lasso_selection(SelectionPipeline):
    &#34;&#34;&#34;
    A stack of Lasso_bisection_selection. Because of collinearity, if there are a batch of featres with high corelation, only one of them will remain.    
    That leads to diffirent behavior between select k features in a time and select k//n features in n times.    
    &#34;&#34;&#34;

    def __init__(self, k, objective=&#34;regression&#34;):
        &#34;&#34;&#34;
        Args:
            objective (str, optional): one of {&#34;Regression&#34;, &#34;BinaryClassification&#34;}
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.name = &#34;multi_Lasso&#34;
        self.objective = objective

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            &#34; Warning&#34;] = &#34;We do not have a reference and this method&#39;s effectivity has not been proven yet.&#34;
        return refer

    def Select(self, x, y, n=5):
        &#34;&#34;&#34;
        Select k//n features for n times, and then concatenate the results.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.
            n (int, optional): Number of batch which splits k to select. Defaults to 10.

        Returns:
            pandas.Series: The score for k selected features. May less than k.
            
        &#34;&#34;&#34;
        result = []
        if self.k == -1:
            self.k = x.shape[0]
        batch_size = self.k // n + 1

        for i in range(n):
            result.append(
                Lasso_bisection_selection(k=batch_size,
                                          objective=self.objective).Select(
                                              x, y))
            x = x.drop(result[-1].index, axis=1)
            if x.shape[1] == 0:
                break
        result = pd.concat(result)
        result = result - result.min()
        result.name = self.name

        self.selected_score = result.sort_values(ascending=False).head(self.k)
        return self.selected_score.copy()


class SVM_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using the support vector of linear support vector classifier as scoring method.    

    SVM_selection is scale sensitive in result.    

    &lt;&lt;Feature Ranking Using Linear SVM&gt;&gt; section 3.2

    &#34;&#34;&#34;

    def __init__(self, k):
        super().__init__(k=k)
        self.kernel = LinearSVC(dual=&#34;auto&#34;,
                                class_weight=&#34;balanced&#34;,
                                random_state=142)
        self.name = &#34;SVM&#34;

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html&#34;
        refer[
            &#34; publication - section 3.2&#34;] = &#34;https://www.csie.ntu.edu.tw/~cjlin/papers/causality.pdf&#34;
        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using the support vector of linear support vector classifier as scoring method.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        self.kernel.fit(x, y)
        svm_weights = np.abs(self.kernel.coef_).sum(axis=0)
        svm_weights /= svm_weights.sum()

        self.scores = pd.Series(svm_weights, index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()


class DT_selection(SelectionPipeline):
    &#34;&#34;&#34;
    A child class of SelectionPipeline.

    Using Decision stump (a single Decision tree) to scoring features.    
    What we do here is:    
        1. normalize data    
        2. transform data into frequency domain by binning through a certain column and applying value_counts    
        3. estimate entropy    
        4. compute c4.5    
    &#34;&#34;&#34;

    def __init__(self, k, bins=10, q=0.05, strategy=&#34;c45&#34;):
        &#34;&#34;&#34;
        Args:
            bins (int, optional): Bins to esimate data distribution entropy. Defaults to 10.
            q (float, optional): Clip data values out of [q, 1-q] percentile to reduce the affect of outliers while estimate entropy. Defaults to 0.05.
            strategy (str, optional): One of {&#34;id3&#34;, &#34;c45&#34;}. The strategy to build decision tree. Defaults to &#34;c45&#34;.
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.bins = bins - 1
        self.q = q
        self.strategy = strategy
        self.name = &#34;DT_score_&#34; + self.strategy

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[self.name() +
              &#34; document&#34;] = &#34;PineBioML.selection.classification.DT_selection&#34;
        refer[
            &#34; publication&#34;] = &#34;Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993.&#34;
        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using Decision stump (a single Decision tree) to scoring features. Though, single layer stump is equivalent to compare the id3/c4.5 score directly.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        upper = x.quantile(1 - self.q)
        lower = x.quantile(self.q)
        normed = (x - lower) / (upper - lower + 1e-5)
        normed = normed.clip(0, 1)
        bin_idx = (normed * self.bins - 0.5).round().astype(np.int32)
        columns = bin_idx.columns

        bin_idx[&#34;label&#34;] = y

        scores = []
        for i in tqdm(columns):
            feature_hists = bin_idx[[i, &#34;label&#34;]].groupby(i)
            feature_entropy = feature_hists.apply(self.entropy)
            feature_size = feature_hists.apply(len) + 1e-3

            info = (feature_entropy / feature_size).sum()
            gain = 0 - info
            if self.strategy == &#34;c45&#34;:
                freq = bin_idx[i].value_counts()
                p = freq / freq.sum()
                split_info = -p * np.log(p)
                gain /= split_info.sum()
            scores.append(gain)
        scores = pd.Series(scores, index=columns,
                           name=self.name).sort_values(ascending=False)
        scores = scores - scores.min()
        return scores

    def entropy(self, x):
        &#34;&#34;&#34;
        Estimate entropy

        Args:
            x (pandas.DataFrame): data with bined label

        Returns:
            float: entropy
        &#34;&#34;&#34;
        label_nums = x[&#34;label&#34;].value_counts()
        label_prop = label_nums / label_nums.sum()

        entropy = -(label_prop * np.log(label_prop + 1e-6)).sum()

        return entropy


class RF_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using random forest to scoring features by gini/entropy gain.    
    We do not provide permutation importance(VI, variable importance) here.

    &#34;&#34;&#34;

    def __init__(self, k, trees=1024, unbalanced=True, strategy=&#34;gini&#34;):
        &#34;&#34;&#34;
        Args:
            trees (int, optional): Number of trees. Defaults to 1024*16.
            strategy (str, optional): Scoring strategy, one of {&#34;gini&#34;, &#34;entropy&#34;}. Defaults to &#34;gini&#34;.
            unbalanced (bool, optional): True to imply class weight to samples. Defaults to True.
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.strategy = strategy
        if unbalanced:
            class_weight = &#34;balanced&#34;
        else:
            class_weight = None

        with parallel_backend(&#39;loky&#39;):
            self.kernel = RandomForestClassifier(n_estimators=trees,
                                                 n_jobs=-1,
                                                 max_samples=0.75,
                                                 class_weight=class_weight,
                                                 criterion=strategy,
                                                 verbose=0,
                                                 ccp_alpha=1e-2,
                                                 random_state=142)
        self.name = &#34;RandomForest_&#34; + self.strategy

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[self.name() + &#34; document&#34;] = &#34;&#34;
        refer[
            &#34; publication cons&#34;] = &#34;https://link.springer.com/article/10.1186/1471-2105-8-25&#34;
        refer[
            &#34; publication pros 1&#34;] = &#34;https://link.springer.com/article/10.1186/1471-2105-10-213&#34;
        refer[
            &#34; publication pros 2&#34;] = &#34;https://www.sciencedirect.com/science/article/pii/S0167947307003076&#34;
        refer[
            &#34; publication survey&#34;] = &#34;https://www.cs.cmu.edu/~qyj/papersA08/11-rfbook.pdf&#34;

        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using random forest to scoring (gini impurity / entropy) features.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        with parallel_backend(&#39;loky&#39;):
            self.kernel.fit(x, y)
        score = self.kernel.feature_importances_
        self.scores = pd.Series(score, index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()


class XGboost_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using XGboost to scoring (gini impurity / entropy) features.

    Warning: If data is too easy, boosting methods is difficult to give score to all features.
    &#34;&#34;&#34;

    def __init__(self, k, unbalanced=True):
        &#34;&#34;&#34;
        Args:
            unbalanced (bool, optional): True to imply class weight to samples. Defaults to False.
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.unbalanced = unbalanced

        self.kernel = xgb.XGBClassifier(random_state=142, subsample=0.7)
        self.name = &#34;XGboost&#34;

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor.feature_importances_&#34;
        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using XGboost to scoring (gini impurity / entropy) features.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        if self.unbalanced:
            sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
        else:
            sample_weight = np.ones(len(y))

        y = OneHotEncoder(sparse_output=False).fit_transform(
            y.to_numpy().reshape(-1, 1))
        self.kernel.fit(x, y, sample_weight=sample_weight)
        score = self.kernel.feature_importances_
        self.scores = pd.Series(score, index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()


class Lightgbm_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using Lightgbm to scoring (gini impurity / entropy) features. 

    Warning: If data is too easy, boosting methods is difficult to give score to all features.
    &#34;&#34;&#34;

    def __init__(self, k, unbalanced=True):
        &#34;&#34;&#34;
        Args:
            unbalanced (bool, optional): True to imply class weight to samples. Defaults to False.
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.unbalanced = unbalanced

        self.kernel = lgbm.LGBMClassifier(learning_rate=0.01,
                                          random_state=142,
                                          subsample=0.7,
                                          subsample_freq=1,
                                          verbosity=-1)
        self.name = &#34;Lightgbm&#34;

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://lightgbm.readthedocs.io/en/latest/Parameters.html#saved_feature_importance_type&#34;
        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using Lightgbm to scoring (gini impurity / entropy) features.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        if self.unbalanced:
            sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
        else:
            sample_weight = np.ones(len(y))

        self.kernel.fit(x, y, sample_weight=sample_weight)
        score = self.kernel.feature_importances_
        self.scores = pd.Series(score, index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()


class AdaBoost_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using AdaBoost to scoring (gini impurity / entropy) features.

    Warning: If data is too easy, boosting methods is difficult to give score to all features.
    &#34;&#34;&#34;

    def __init__(self, k, unbalanced=True, n_iter=128, learning_rate=0.01):
        &#34;&#34;&#34;
        Args:
            n_iter (int, optional): Number of trees also number of iteration to boost. Defaults to 64.
            learning_rate (float, optional): boosting learning rate. Defaults to 0.01.
            unbalanced (bool, optional): True to imply class weight to samples. Defaults to False.
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.unbalanced = unbalanced
        self.kernel = AdaBoostClassifier(
            n_estimators=n_iter,
            learning_rate=learning_rate,
            random_state=142,
            algorithm=&#34;SAMME&#34;,
        )
        self.name = &#34;AdaBoost&#34; + str(n_iter)

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier.feature_importances_&#34;
        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using AdaBoost to scoring (gini impurity / entropy) features.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        print(&#34;I don&#39;t have a progress bar but I am running&#34;)
        if self.unbalanced:
            sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
        else:
            sample_weight = np.ones(len(y))
        self.kernel.fit(x, y, sample_weight)
        score = self.kernel.feature_importances_
        self.scores = pd.Series(score, index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()


class essemble_selector(SelectionPipeline):
    &#34;&#34;&#34;
    A functional stack of diffirent methods.    
    What we do here is:    
        1. calculate feature importance in different methods.    
        2. standardize the scores and then averaging through methods.    
        3. If z_importance_threshold not None, then all features with averaging score higher than z_importance_threshold will be selected.    
           else top k feature with averaging score will be selected.    
    &#34;&#34;&#34;

    def __init__(self,
                 k=-1,
                 RF_trees=1024,
                 z_importance_threshold: int = None):
        &#34;&#34;&#34;

        Args:
            k (int, optional): The number of features to be selected. Defaults to -1.
            RF_trees (int, optional): number of trees using for randomforest. Defaults to 1024.
            z_importance_threshold (int, optional): The threshold to picking features. Defaults to None.
        
        Todo:
            auto adjust the RF_Trees by number of input features.
        &#34;&#34;&#34;
        self.k = k
        self.z_importance_threshold = z_importance_threshold

        self.kernels = {
            &#34;c45&#34;: DT_selection(k=k, strategy=&#34;c45&#34;),
            &#34;RF_gini&#34;: RF_selection(k=k, strategy=&#34;gini&#34;, trees=RF_trees),
            &#34;Lasso_Bisection&#34;: Lasso_bisection_selection(k=k),
            &#34;multi_Lasso&#34;: multi_Lasso_selection(k=k),
            &#34;SVM&#34;: SVM_selection(k=k),
            &#34;AdaBoost&#34;: AdaBoost_selection(k=k),
            &#34;XGboost&#34;: XGboost_selection(k=k),
            &#34;Lightgbm&#34;: Lightgbm_selection(k=k)
        }

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; WARNING&#34;] = &#34;This method has no reference yet. That means the effectivity has not been proven yet. It somehow works in experience.&#34;
        return refer

    def Select(self, x, y):
        &#34;&#34;&#34;
        Calling all the methods in kernel sequancially.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.
            k (int): Number of feature to select. The result may less than k

        Returns:
            pandas.Series: The concatenated results. Top k (may less than k) important feature from diffient methods.
        &#34;&#34;&#34;
        results = []
        for method in self.kernels:
            print(&#34;Using &#34;, method, &#34; to select.&#34;)
            results.append(self.kernels[method].Select(x.copy(), y))
            print(method, &#34; is done.\n&#34;)

        name = pd.concat([pd.Series(i.index, name=i.name) for i in results],
                         axis=1)
        importance = pd.concat(results, axis=1)
        self.selected_score = importance
        return name, importance

    def fit(self, x, y):
        &#34;&#34;&#34;
        sklearn api

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods.
        &#34;&#34;&#34;
        name, importance = self.Select(x, y)

        return self

    def transform(self, x):
        z_scores = (self.selected_score - self.selected_score.mean()) / (
            self.selected_score.std() + 1e-4)
        z_scores = z_scores.mean(axis=1).sort_values(ascending=False)

        if self.z_importance_threshold is None:
            return x[z_scores.index[:self.k]]
        else:
            return x[z_scores[z_scores &gt; self.z_importance_threshold].index]

    def fit_transform(self, x, y):
        self.fit(x, y)
        return self.transform(x)

    def plotting(self):
        for method in self.kernels:
            self.kernels[method].plotting()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="PineBioML.selection.classification.AdaBoost_selection"><code class="flex name class">
<span>class <span class="ident">AdaBoost_selection</span></span>
<span>(</span><span>k, unbalanced=True, n_iter=128, learning_rate=0.01)</span>
</code></dt>
<dd>
<div class="desc"><p>Using AdaBoost to scoring (gini impurity / entropy) features.</p>
<p>Warning: If data is too easy, boosting methods is difficult to give score to all features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_iter</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of trees also number of iteration to boost. Defaults to 64.</dd>
<dt><strong><code>learning_rate</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>boosting learning rate. Defaults to 0.01.</dd>
<dt><strong><code>unbalanced</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>True to imply class weight to samples. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AdaBoost_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using AdaBoost to scoring (gini impurity / entropy) features.

    Warning: If data is too easy, boosting methods is difficult to give score to all features.
    &#34;&#34;&#34;

    def __init__(self, k, unbalanced=True, n_iter=128, learning_rate=0.01):
        &#34;&#34;&#34;
        Args:
            n_iter (int, optional): Number of trees also number of iteration to boost. Defaults to 64.
            learning_rate (float, optional): boosting learning rate. Defaults to 0.01.
            unbalanced (bool, optional): True to imply class weight to samples. Defaults to False.
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.unbalanced = unbalanced
        self.kernel = AdaBoostClassifier(
            n_estimators=n_iter,
            learning_rate=learning_rate,
            random_state=142,
            algorithm=&#34;SAMME&#34;,
        )
        self.name = &#34;AdaBoost&#34; + str(n_iter)

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier.feature_importances_&#34;
        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using AdaBoost to scoring (gini impurity / entropy) features.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        print(&#34;I don&#39;t have a progress bar but I am running&#34;)
        if self.unbalanced:
            sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
        else:
            sample_weight = np.ones(len(y))
        self.kernel.fit(x, y, sample_weight)
        score = self.kernel.feature_importances_
        self.scores = pd.Series(score, index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.selection.classification.AdaBoost_selection.Scoring"><code class="name flex">
<span>def <span class="ident">Scoring</span></span>(<span>self, x, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Using AdaBoost to scoring (gini impurity / entropy) features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>a 2D array</code></dt>
<dd>The data to extract information.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>a 1D array</code></dt>
<dd>The target label for methods. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Series</code> or <code>pandas.DataFrame</code></dt>
<dd>The score for each feature. Some elements may be empty.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Scoring(self, x, y=None):
    &#34;&#34;&#34;
    Using AdaBoost to scoring (gini impurity / entropy) features.

    Args:
        x (pandas.DataFrame or a 2D array): The data to extract information.
        y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

    Returns:
        pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
    &#34;&#34;&#34;
    print(&#34;I don&#39;t have a progress bar but I am running&#34;)
    if self.unbalanced:
        sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
    else:
        sample_weight = np.ones(len(y))
    self.kernel.fit(x, y, sample_weight)
    score = self.kernel.feature_importances_
    self.scores = pd.Series(score, index=x.columns,
                            name=self.name).sort_values(ascending=False)
    return self.scores.copy()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></b></code>:
<ul class="hlist">
<li><code><a title="PineBioML.selection.SelectionPipeline.Choose" href="index.html#PineBioML.selection.SelectionPipeline.Choose">Choose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Diagnose" href="index.html#PineBioML.selection.SelectionPipeline.Diagnose">Diagnose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Plotting" href="index.html#PineBioML.selection.SelectionPipeline.Plotting">Plotting</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Report" href="index.html#PineBioML.selection.SelectionPipeline.Report">Report</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Select" href="index.html#PineBioML.selection.SelectionPipeline.Select">Select</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.fit" href="index.html#PineBioML.selection.SelectionPipeline.fit">fit</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.reference" href="index.html#PineBioML.selection.SelectionPipeline.reference">reference</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="PineBioML.selection.classification.DT_selection"><code class="flex name class">
<span>class <span class="ident">DT_selection</span></span>
<span>(</span><span>k, bins=10, q=0.05, strategy='c45')</span>
</code></dt>
<dd>
<div class="desc"><p>A child class of SelectionPipeline.</p>
<p>Using Decision stump (a single Decision tree) to scoring features.
<br>
What we do here is:
<br>
1. normalize data
<br>
2. transform data into frequency domain by binning through a certain column and applying value_counts
<br>
3. estimate entropy
<br>
4. compute c4.5
</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>bins</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Bins to esimate data distribution entropy. Defaults to 10.</dd>
<dt><strong><code>q</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Clip data values out of [q, 1-q] percentile to reduce the affect of outliers while estimate entropy. Defaults to 0.05.</dd>
<dt><strong><code>strategy</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>One of {"id3", "c45"}. The strategy to build decision tree. Defaults to "c45".</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DT_selection(SelectionPipeline):
    &#34;&#34;&#34;
    A child class of SelectionPipeline.

    Using Decision stump (a single Decision tree) to scoring features.    
    What we do here is:    
        1. normalize data    
        2. transform data into frequency domain by binning through a certain column and applying value_counts    
        3. estimate entropy    
        4. compute c4.5    
    &#34;&#34;&#34;

    def __init__(self, k, bins=10, q=0.05, strategy=&#34;c45&#34;):
        &#34;&#34;&#34;
        Args:
            bins (int, optional): Bins to esimate data distribution entropy. Defaults to 10.
            q (float, optional): Clip data values out of [q, 1-q] percentile to reduce the affect of outliers while estimate entropy. Defaults to 0.05.
            strategy (str, optional): One of {&#34;id3&#34;, &#34;c45&#34;}. The strategy to build decision tree. Defaults to &#34;c45&#34;.
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.bins = bins - 1
        self.q = q
        self.strategy = strategy
        self.name = &#34;DT_score_&#34; + self.strategy

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[self.name() +
              &#34; document&#34;] = &#34;PineBioML.selection.classification.DT_selection&#34;
        refer[
            &#34; publication&#34;] = &#34;Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993.&#34;
        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using Decision stump (a single Decision tree) to scoring features. Though, single layer stump is equivalent to compare the id3/c4.5 score directly.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        upper = x.quantile(1 - self.q)
        lower = x.quantile(self.q)
        normed = (x - lower) / (upper - lower + 1e-5)
        normed = normed.clip(0, 1)
        bin_idx = (normed * self.bins - 0.5).round().astype(np.int32)
        columns = bin_idx.columns

        bin_idx[&#34;label&#34;] = y

        scores = []
        for i in tqdm(columns):
            feature_hists = bin_idx[[i, &#34;label&#34;]].groupby(i)
            feature_entropy = feature_hists.apply(self.entropy)
            feature_size = feature_hists.apply(len) + 1e-3

            info = (feature_entropy / feature_size).sum()
            gain = 0 - info
            if self.strategy == &#34;c45&#34;:
                freq = bin_idx[i].value_counts()
                p = freq / freq.sum()
                split_info = -p * np.log(p)
                gain /= split_info.sum()
            scores.append(gain)
        scores = pd.Series(scores, index=columns,
                           name=self.name).sort_values(ascending=False)
        scores = scores - scores.min()
        return scores

    def entropy(self, x):
        &#34;&#34;&#34;
        Estimate entropy

        Args:
            x (pandas.DataFrame): data with bined label

        Returns:
            float: entropy
        &#34;&#34;&#34;
        label_nums = x[&#34;label&#34;].value_counts()
        label_prop = label_nums / label_nums.sum()

        entropy = -(label_prop * np.log(label_prop + 1e-6)).sum()

        return entropy</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.selection.classification.DT_selection.Scoring"><code class="name flex">
<span>def <span class="ident">Scoring</span></span>(<span>self, x, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Using Decision stump (a single Decision tree) to scoring features. Though, single layer stump is equivalent to compare the id3/c4.5 score directly.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>a 2D array</code></dt>
<dd>The data to extract information.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>a 1D array</code></dt>
<dd>The target label for methods. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Series</code> or <code>pandas.DataFrame</code></dt>
<dd>The score for each feature. Some elements may be empty.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Scoring(self, x, y=None):
    &#34;&#34;&#34;
    Using Decision stump (a single Decision tree) to scoring features. Though, single layer stump is equivalent to compare the id3/c4.5 score directly.

    Args:
        x (pandas.DataFrame or a 2D array): The data to extract information.
        y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

    Returns:
        pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
    &#34;&#34;&#34;
    upper = x.quantile(1 - self.q)
    lower = x.quantile(self.q)
    normed = (x - lower) / (upper - lower + 1e-5)
    normed = normed.clip(0, 1)
    bin_idx = (normed * self.bins - 0.5).round().astype(np.int32)
    columns = bin_idx.columns

    bin_idx[&#34;label&#34;] = y

    scores = []
    for i in tqdm(columns):
        feature_hists = bin_idx[[i, &#34;label&#34;]].groupby(i)
        feature_entropy = feature_hists.apply(self.entropy)
        feature_size = feature_hists.apply(len) + 1e-3

        info = (feature_entropy / feature_size).sum()
        gain = 0 - info
        if self.strategy == &#34;c45&#34;:
            freq = bin_idx[i].value_counts()
            p = freq / freq.sum()
            split_info = -p * np.log(p)
            gain /= split_info.sum()
        scores.append(gain)
    scores = pd.Series(scores, index=columns,
                       name=self.name).sort_values(ascending=False)
    scores = scores - scores.min()
    return scores</code></pre>
</details>
</dd>
<dt id="PineBioML.selection.classification.DT_selection.entropy"><code class="name flex">
<span>def <span class="ident">entropy</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimate entropy</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>data with bined label</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>entropy</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entropy(self, x):
    &#34;&#34;&#34;
    Estimate entropy

    Args:
        x (pandas.DataFrame): data with bined label

    Returns:
        float: entropy
    &#34;&#34;&#34;
    label_nums = x[&#34;label&#34;].value_counts()
    label_prop = label_nums / label_nums.sum()

    entropy = -(label_prop * np.log(label_prop + 1e-6)).sum()

    return entropy</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></b></code>:
<ul class="hlist">
<li><code><a title="PineBioML.selection.SelectionPipeline.Choose" href="index.html#PineBioML.selection.SelectionPipeline.Choose">Choose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Diagnose" href="index.html#PineBioML.selection.SelectionPipeline.Diagnose">Diagnose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Plotting" href="index.html#PineBioML.selection.SelectionPipeline.Plotting">Plotting</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Report" href="index.html#PineBioML.selection.SelectionPipeline.Report">Report</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Select" href="index.html#PineBioML.selection.SelectionPipeline.Select">Select</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.fit" href="index.html#PineBioML.selection.SelectionPipeline.fit">fit</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.reference" href="index.html#PineBioML.selection.SelectionPipeline.reference">reference</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="PineBioML.selection.classification.Lasso_bisection_selection"><code class="flex name class">
<span>class <span class="ident">Lasso_bisection_selection</span></span>
<span>(</span><span>k, unbalanced=True, objective='regression')</span>
</code></dt>
<dd>
<div class="desc"><p>Using Lasso (L1 penalty) regression as scoring method.
More specifically, L1 penalty will force feature weights to be zeros.
As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.</p>
<p>Lasso_bisection_selection will use binary search to find out when all weights vanish.</p>
<p>The trace of weight vanishment is not support.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>unbalanced</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>False to imply class weight to samples. Defaults to True.</dd>
<dt><strong><code>objective</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>one of {"Regression", "BinaryClassification"}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Lasso_bisection_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using Lasso (L1 penalty) regression as scoring method.  More specifically, L1 penalty will force feature weights to be zeros. 
    As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.

    Lasso_bisection_selection will use binary search to find out when all weights vanish.
    
    The trace of weight vanishment is not support.

    &#34;&#34;&#34;

    def __init__(self, k, unbalanced=True, objective=&#34;regression&#34;):
        &#34;&#34;&#34;
        Args:
            unbalanced (bool, optional): False to imply class weight to samples. Defaults to True.
            objective (str, optional): one of {&#34;Regression&#34;, &#34;BinaryClassification&#34;}
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.upper_init = 1e+5
        self.lower_init = 1e-5
        self.objective = objective
        if self.objective in [&#34;regression&#34;, &#34;Regression&#34;]:
            self.regression = True
        else:
            self.regression = False
        self.unbalanced = unbalanced
        self.name = &#34;LassoLinear&#34;

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34;
        refer[
            &#34; publication 1&#34;] = &#34;https://projecteuclid.org/journals/annals-of-statistics/volume-37/issue-5A/High-dimensional-variable-selection/10.1214/08-AOS646.full&#34;
        refer[
            &#34; publication 2&#34;] = &#34;https://www.tandfonline.com/doi/abs/10.1198/016214506000000735?casa_token=5HDhtyCfh40AAAAA:4NxSU97CZubZVpReaQNsSBpqA10_xNhspTQobPnb_z2YXe3Wf-HBHV8OygbqUkmJQPt2Jmp7ZlJPWd0&#34;
        return refer

    def create_kernel(self, C):
        if self.regression:
            return Lasso(alpha=C)
        else:
            return LogisticRegression(penalty=&#34;l1&#34;,
                                      C=1 / C,
                                      solver=&#34;liblinear&#34;,
                                      random_state=142)

    def Select(self, x, y):
        &#34;&#34;&#34;
        Using Lasso (L1 penalty) regression as scoring method.  More specifically, L1 penalty will force feature weights to be zeros. 
        As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.

        Lasso_bisection_selection will use binary search to find out when all weights vanish.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.
            k (int): Number of feature to select. The result may less than k

        Returns:
            pandas.Series: The score for k selected features. May less than k.
        &#34;&#34;&#34;

        # train test split
        X_train = x.copy()
        y_train = y.copy()

        if self.unbalanced:
            weights = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y_train)
        else:
            weights = np.ones_like(y_train)

        if self.k == -1:
            self.k = x.shape[0]

        y_train = OneHotEncoder(sparse_output=False).fit_transform(
            y_train.to_numpy().reshape(-1, 1))

        lassoes = []
        # Bisection searching
        ### standardize x
        X_train = X_train / X_train.values.std()

        upper = self.upper_init
        lassoes.append(self.create_kernel(C=upper))
        lassoes[-1].fit(X_train, y_train, weights)
        upper_alive = (self.coef_to_importance(lassoes[-1].coef_) != 0).sum()
        #print(upper, upper_alive)

        lower = self.lower_init
        lassoes.append(self.create_kernel(C=lower))
        lassoes[-1].fit(X_train, y_train, weights)
        lower_alive = (self.coef_to_importance(lassoes[-1].coef_) != 0).sum()
        #print(lower, lower_alive)

        counter = 0
        while not lower_alive == self.k:
            alpha = (upper + lower) / 2
            lassoes.append(self.create_kernel(C=alpha))
            lassoes[-1].fit(X_train, y_train, weights)
            alive = (self.coef_to_importance(lassoes[-1].coef_) != 0).sum()
            #print(alive, alpha)

            if alive &gt;= self.k:
                lower = alpha
                lower_alive = alive
            else:
                upper = alpha
                upper_alive = alive

            counter += 1
            if counter &gt; 40:
                break

        coef = np.array([clr.coef_ for clr in lassoes])

        self.scores = pd.Series(self.coef_to_importance(coef[-1]),
                                index=x.columns,
                                name=self.name).sort_values(ascending=False)
        self.selected_score = self.scores.head(self.k)
        return self.selected_score

    def coef_to_importance(self, coef):
        return np.linalg.norm(coef, ord=2, axis=0)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.selection.classification.Lasso_bisection_selection.Select"><code class="name flex">
<span>def <span class="ident">Select</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Using Lasso (L1 penalty) regression as scoring method.
More specifically, L1 penalty will force feature weights to be zeros.
As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.</p>
<p>Lasso_bisection_selection will use binary search to find out when all weights vanish.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>a 2D array</code></dt>
<dd>The data to extract information.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>a 1D array</code></dt>
<dd>The target label for methods. Defaults to None.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of feature to select. The result may less than k</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Series</code></dt>
<dd>The score for k selected features. May less than k.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Select(self, x, y):
    &#34;&#34;&#34;
    Using Lasso (L1 penalty) regression as scoring method.  More specifically, L1 penalty will force feature weights to be zeros. 
    As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.

    Lasso_bisection_selection will use binary search to find out when all weights vanish.

    Args:
        x (pandas.DataFrame or a 2D array): The data to extract information.
        y (pandas.Series or a 1D array): The target label for methods. Defaults to None.
        k (int): Number of feature to select. The result may less than k

    Returns:
        pandas.Series: The score for k selected features. May less than k.
    &#34;&#34;&#34;

    # train test split
    X_train = x.copy()
    y_train = y.copy()

    if self.unbalanced:
        weights = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y_train)
    else:
        weights = np.ones_like(y_train)

    if self.k == -1:
        self.k = x.shape[0]

    y_train = OneHotEncoder(sparse_output=False).fit_transform(
        y_train.to_numpy().reshape(-1, 1))

    lassoes = []
    # Bisection searching
    ### standardize x
    X_train = X_train / X_train.values.std()

    upper = self.upper_init
    lassoes.append(self.create_kernel(C=upper))
    lassoes[-1].fit(X_train, y_train, weights)
    upper_alive = (self.coef_to_importance(lassoes[-1].coef_) != 0).sum()
    #print(upper, upper_alive)

    lower = self.lower_init
    lassoes.append(self.create_kernel(C=lower))
    lassoes[-1].fit(X_train, y_train, weights)
    lower_alive = (self.coef_to_importance(lassoes[-1].coef_) != 0).sum()
    #print(lower, lower_alive)

    counter = 0
    while not lower_alive == self.k:
        alpha = (upper + lower) / 2
        lassoes.append(self.create_kernel(C=alpha))
        lassoes[-1].fit(X_train, y_train, weights)
        alive = (self.coef_to_importance(lassoes[-1].coef_) != 0).sum()
        #print(alive, alpha)

        if alive &gt;= self.k:
            lower = alpha
            lower_alive = alive
        else:
            upper = alpha
            upper_alive = alive

        counter += 1
        if counter &gt; 40:
            break

    coef = np.array([clr.coef_ for clr in lassoes])

    self.scores = pd.Series(self.coef_to_importance(coef[-1]),
                            index=x.columns,
                            name=self.name).sort_values(ascending=False)
    self.selected_score = self.scores.head(self.k)
    return self.selected_score</code></pre>
</details>
</dd>
<dt id="PineBioML.selection.classification.Lasso_bisection_selection.coef_to_importance"><code class="name flex">
<span>def <span class="ident">coef_to_importance</span></span>(<span>self, coef)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coef_to_importance(self, coef):
    return np.linalg.norm(coef, ord=2, axis=0)</code></pre>
</details>
</dd>
<dt id="PineBioML.selection.classification.Lasso_bisection_selection.create_kernel"><code class="name flex">
<span>def <span class="ident">create_kernel</span></span>(<span>self, C)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_kernel(self, C):
    if self.regression:
        return Lasso(alpha=C)
    else:
        return LogisticRegression(penalty=&#34;l1&#34;,
                                  C=1 / C,
                                  solver=&#34;liblinear&#34;,
                                  random_state=142)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></b></code>:
<ul class="hlist">
<li><code><a title="PineBioML.selection.SelectionPipeline.Choose" href="index.html#PineBioML.selection.SelectionPipeline.Choose">Choose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Diagnose" href="index.html#PineBioML.selection.SelectionPipeline.Diagnose">Diagnose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Plotting" href="index.html#PineBioML.selection.SelectionPipeline.Plotting">Plotting</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Report" href="index.html#PineBioML.selection.SelectionPipeline.Report">Report</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Scoring" href="index.html#PineBioML.selection.SelectionPipeline.Scoring">Scoring</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.fit" href="index.html#PineBioML.selection.SelectionPipeline.fit">fit</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.reference" href="index.html#PineBioML.selection.SelectionPipeline.reference">reference</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="PineBioML.selection.classification.Lasso_selection"><code class="flex name class">
<span>class <span class="ident">Lasso_selection</span></span>
<span>(</span><span>k, unbalanced=True, objective='Regression')</span>
</code></dt>
<dd>
<div class="desc"><p>Using Lasso (L1 penalty) regression as scoring method.
More specifically, L1 penalty will force feature weights to be zeros.
As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.</p>
<p>Lasso_selection will use grid search to find out when all weights vanish.</p>
<p>Lasso_selection is scale sensitive in numerical and in result.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>unbalanced</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>False to imply class weight to samples. Defaults to True.</dd>
<dt><strong><code>objective</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>one of {"Regression", "BinaryClassification"}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Lasso_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using Lasso (L1 penalty) regression as scoring method.  More specifically, L1 penalty will force feature weights to be zeros. 
    As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.

    Lasso_selection will use grid search to find out when all weights vanish.

    Lasso_selection is scale sensitive in numerical and in result.

    &#34;&#34;&#34;

    def __init__(self, k, unbalanced=True, objective=&#34;Regression&#34;):
        &#34;&#34;&#34;
        Args:
            unbalanced (bool, optional): False to imply class weight to samples. Defaults to True.
            objective (str, optional): one of {&#34;Regression&#34;, &#34;BinaryClassification&#34;}
        &#34;&#34;&#34;
        super().__init__(k=k)

        # parameters
        self.objective = objective
        if self.objective in [&#34;regression&#34;, &#34;Regression&#34;]:
            self.regression = True
        else:
            self.regression = False
        self.da = 0.025  # d alpha
        self.upper_init = 50
        self.unbalanced = unbalanced
        self.name = &#34;LassoLinear&#34;

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34;
        refer[
            &#34; publication 1&#34;] = &#34;https://projecteuclid.org/journals/annals-of-statistics/volume-37/issue-5A/High-dimensional-variable-selection/10.1214/08-AOS646.full&#34;
        refer[
            &#34; publication 2&#34;] = &#34;https://www.tandfonline.com/doi/abs/10.1198/016214506000000735?casa_token=5HDhtyCfh40AAAAA:4NxSU97CZubZVpReaQNsSBpqA10_xNhspTQobPnb_z2YXe3Wf-HBHV8OygbqUkmJQPt2Jmp7ZlJPWd0&#34;
        return refer

    def create_kernel(self, C):
        &#34;&#34;&#34;
        Create diffirent kernel according to opjective.

        Args:
            C (float): The coefficient to L1 penalty.

        Returns:
            sklearn.linearmodel: a kernel of sklearn linearmodel
        &#34;&#34;&#34;
        if self.regression:
            return Lasso(alpha=C)
        else:
            return LogisticRegression(penalty=&#34;l1&#34;,
                                      C=1 / C,
                                      solver=&#34;liblinear&#34;,
                                      random_state=142)

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using Lasso (L1 penalty) regression as scoring method.  More specifically, L1 penalty will force feature weights to be zeros. 
        As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.

        Lasso_selection will use grid search to find out when all weights vanish.

         Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        
        To do:
            kfold validation performance threshold.
        
        &#34;&#34;&#34;

        X_train = x.copy()
        y_train = y.copy()

        if self.unbalanced:
            weights = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y_train)
        else:
            weights = np.ones_like(y_train)

        lassoes = []
        # grid searching
        if self.regression:
            grids = np.arange(self.da, self.upper_init, self.da)
        else:
            grids = np.arange(self.upper_init, self.da, -self.da)

        for alpha in tqdm(grids):
            lassoes.append(self.create_kernel(C=alpha))
            lassoes[-1].fit(X_train, y_train, weights)
            alive = (lassoes[-1].coef_ != 0).sum()

            if alive &lt; 1:
                print(&#34;all coefficient are dead, terminated.&#34;)
                break

        coef = np.array([clr.coef_ for clr in lassoes]).flatten()

        self.scores = pd.Series(np.logical_not(coef == 0).sum(axis=0) *
                                self.da,
                                index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.selection.classification.Lasso_selection.Scoring"><code class="name flex">
<span>def <span class="ident">Scoring</span></span>(<span>self, x, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Using Lasso (L1 penalty) regression as scoring method.
More specifically, L1 penalty will force feature weights to be zeros.
As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.</p>
<p>Lasso_selection will use grid search to find out when all weights vanish.</p>
<p>Args:
x (pandas.DataFrame or a 2D array): The data to extract information.
y (pandas.Series or a 1D array): The target label for methods. Defaults to None.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Series</code> or <code>pandas.DataFrame</code></dt>
<dd>The score for each feature. Some elements may be empty.</dd>
</dl>
<p>To do:
kfold validation performance threshold.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Scoring(self, x, y=None):
    &#34;&#34;&#34;
    Using Lasso (L1 penalty) regression as scoring method.  More specifically, L1 penalty will force feature weights to be zeros. 
    As the coefficient of penalty increases, more and more weights of features got killed and the important feature will remain.

    Lasso_selection will use grid search to find out when all weights vanish.

     Args:
        x (pandas.DataFrame or a 2D array): The data to extract information.
        y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

    Returns:
        pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
    
    To do:
        kfold validation performance threshold.
    
    &#34;&#34;&#34;

    X_train = x.copy()
    y_train = y.copy()

    if self.unbalanced:
        weights = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y_train)
    else:
        weights = np.ones_like(y_train)

    lassoes = []
    # grid searching
    if self.regression:
        grids = np.arange(self.da, self.upper_init, self.da)
    else:
        grids = np.arange(self.upper_init, self.da, -self.da)

    for alpha in tqdm(grids):
        lassoes.append(self.create_kernel(C=alpha))
        lassoes[-1].fit(X_train, y_train, weights)
        alive = (lassoes[-1].coef_ != 0).sum()

        if alive &lt; 1:
            print(&#34;all coefficient are dead, terminated.&#34;)
            break

    coef = np.array([clr.coef_ for clr in lassoes]).flatten()

    self.scores = pd.Series(np.logical_not(coef == 0).sum(axis=0) *
                            self.da,
                            index=x.columns,
                            name=self.name).sort_values(ascending=False)
    return self.scores.copy()</code></pre>
</details>
</dd>
<dt id="PineBioML.selection.classification.Lasso_selection.create_kernel"><code class="name flex">
<span>def <span class="ident">create_kernel</span></span>(<span>self, C)</span>
</code></dt>
<dd>
<div class="desc"><p>Create diffirent kernel according to opjective.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>C</code></strong> :&ensp;<code>float</code></dt>
<dd>The coefficient to L1 penalty.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>sklearn.linearmodel</code></dt>
<dd>a kernel of sklearn linearmodel</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_kernel(self, C):
    &#34;&#34;&#34;
    Create diffirent kernel according to opjective.

    Args:
        C (float): The coefficient to L1 penalty.

    Returns:
        sklearn.linearmodel: a kernel of sklearn linearmodel
    &#34;&#34;&#34;
    if self.regression:
        return Lasso(alpha=C)
    else:
        return LogisticRegression(penalty=&#34;l1&#34;,
                                  C=1 / C,
                                  solver=&#34;liblinear&#34;,
                                  random_state=142)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></b></code>:
<ul class="hlist">
<li><code><a title="PineBioML.selection.SelectionPipeline.Choose" href="index.html#PineBioML.selection.SelectionPipeline.Choose">Choose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Diagnose" href="index.html#PineBioML.selection.SelectionPipeline.Diagnose">Diagnose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Plotting" href="index.html#PineBioML.selection.SelectionPipeline.Plotting">Plotting</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Report" href="index.html#PineBioML.selection.SelectionPipeline.Report">Report</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Select" href="index.html#PineBioML.selection.SelectionPipeline.Select">Select</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.fit" href="index.html#PineBioML.selection.SelectionPipeline.fit">fit</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.reference" href="index.html#PineBioML.selection.SelectionPipeline.reference">reference</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="PineBioML.selection.classification.Lightgbm_selection"><code class="flex name class">
<span>class <span class="ident">Lightgbm_selection</span></span>
<span>(</span><span>k, unbalanced=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Using Lightgbm to scoring (gini impurity / entropy) features. </p>
<p>Warning: If data is too easy, boosting methods is difficult to give score to all features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>unbalanced</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>True to imply class weight to samples. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Lightgbm_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using Lightgbm to scoring (gini impurity / entropy) features. 

    Warning: If data is too easy, boosting methods is difficult to give score to all features.
    &#34;&#34;&#34;

    def __init__(self, k, unbalanced=True):
        &#34;&#34;&#34;
        Args:
            unbalanced (bool, optional): True to imply class weight to samples. Defaults to False.
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.unbalanced = unbalanced

        self.kernel = lgbm.LGBMClassifier(learning_rate=0.01,
                                          random_state=142,
                                          subsample=0.7,
                                          subsample_freq=1,
                                          verbosity=-1)
        self.name = &#34;Lightgbm&#34;

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://lightgbm.readthedocs.io/en/latest/Parameters.html#saved_feature_importance_type&#34;
        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using Lightgbm to scoring (gini impurity / entropy) features.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        if self.unbalanced:
            sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
        else:
            sample_weight = np.ones(len(y))

        self.kernel.fit(x, y, sample_weight=sample_weight)
        score = self.kernel.feature_importances_
        self.scores = pd.Series(score, index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.selection.classification.Lightgbm_selection.Scoring"><code class="name flex">
<span>def <span class="ident">Scoring</span></span>(<span>self, x, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Using Lightgbm to scoring (gini impurity / entropy) features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>a 2D array</code></dt>
<dd>The data to extract information.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>a 1D array</code></dt>
<dd>The target label for methods. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Series</code> or <code>pandas.DataFrame</code></dt>
<dd>The score for each feature. Some elements may be empty.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Scoring(self, x, y=None):
    &#34;&#34;&#34;
    Using Lightgbm to scoring (gini impurity / entropy) features.

    Args:
        x (pandas.DataFrame or a 2D array): The data to extract information.
        y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

    Returns:
        pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
    &#34;&#34;&#34;
    if self.unbalanced:
        sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
    else:
        sample_weight = np.ones(len(y))

    self.kernel.fit(x, y, sample_weight=sample_weight)
    score = self.kernel.feature_importances_
    self.scores = pd.Series(score, index=x.columns,
                            name=self.name).sort_values(ascending=False)
    return self.scores.copy()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></b></code>:
<ul class="hlist">
<li><code><a title="PineBioML.selection.SelectionPipeline.Choose" href="index.html#PineBioML.selection.SelectionPipeline.Choose">Choose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Diagnose" href="index.html#PineBioML.selection.SelectionPipeline.Diagnose">Diagnose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Plotting" href="index.html#PineBioML.selection.SelectionPipeline.Plotting">Plotting</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Report" href="index.html#PineBioML.selection.SelectionPipeline.Report">Report</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Select" href="index.html#PineBioML.selection.SelectionPipeline.Select">Select</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.fit" href="index.html#PineBioML.selection.SelectionPipeline.fit">fit</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.reference" href="index.html#PineBioML.selection.SelectionPipeline.reference">reference</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="PineBioML.selection.classification.RF_selection"><code class="flex name class">
<span>class <span class="ident">RF_selection</span></span>
<span>(</span><span>k, trees=1024, unbalanced=True, strategy='gini')</span>
</code></dt>
<dd>
<div class="desc"><p>Using random forest to scoring features by gini/entropy gain.
<br>
We do not provide permutation importance(VI, variable importance) here.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trees</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of trees. Defaults to 1024*16.</dd>
<dt><strong><code>strategy</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Scoring strategy, one of {"gini", "entropy"}. Defaults to "gini".</dd>
<dt><strong><code>unbalanced</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>True to imply class weight to samples. Defaults to True.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RF_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using random forest to scoring features by gini/entropy gain.    
    We do not provide permutation importance(VI, variable importance) here.

    &#34;&#34;&#34;

    def __init__(self, k, trees=1024, unbalanced=True, strategy=&#34;gini&#34;):
        &#34;&#34;&#34;
        Args:
            trees (int, optional): Number of trees. Defaults to 1024*16.
            strategy (str, optional): Scoring strategy, one of {&#34;gini&#34;, &#34;entropy&#34;}. Defaults to &#34;gini&#34;.
            unbalanced (bool, optional): True to imply class weight to samples. Defaults to True.
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.strategy = strategy
        if unbalanced:
            class_weight = &#34;balanced&#34;
        else:
            class_weight = None

        with parallel_backend(&#39;loky&#39;):
            self.kernel = RandomForestClassifier(n_estimators=trees,
                                                 n_jobs=-1,
                                                 max_samples=0.75,
                                                 class_weight=class_weight,
                                                 criterion=strategy,
                                                 verbose=0,
                                                 ccp_alpha=1e-2,
                                                 random_state=142)
        self.name = &#34;RandomForest_&#34; + self.strategy

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[self.name() + &#34; document&#34;] = &#34;&#34;
        refer[
            &#34; publication cons&#34;] = &#34;https://link.springer.com/article/10.1186/1471-2105-8-25&#34;
        refer[
            &#34; publication pros 1&#34;] = &#34;https://link.springer.com/article/10.1186/1471-2105-10-213&#34;
        refer[
            &#34; publication pros 2&#34;] = &#34;https://www.sciencedirect.com/science/article/pii/S0167947307003076&#34;
        refer[
            &#34; publication survey&#34;] = &#34;https://www.cs.cmu.edu/~qyj/papersA08/11-rfbook.pdf&#34;

        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using random forest to scoring (gini impurity / entropy) features.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        with parallel_backend(&#39;loky&#39;):
            self.kernel.fit(x, y)
        score = self.kernel.feature_importances_
        self.scores = pd.Series(score, index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.selection.classification.RF_selection.Scoring"><code class="name flex">
<span>def <span class="ident">Scoring</span></span>(<span>self, x, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Using random forest to scoring (gini impurity / entropy) features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>a 2D array</code></dt>
<dd>The data to extract information.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>a 1D array</code></dt>
<dd>The target label for methods. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Series</code> or <code>pandas.DataFrame</code></dt>
<dd>The score for each feature. Some elements may be empty.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Scoring(self, x, y=None):
    &#34;&#34;&#34;
    Using random forest to scoring (gini impurity / entropy) features.

    Args:
        x (pandas.DataFrame or a 2D array): The data to extract information.
        y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

    Returns:
        pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
    &#34;&#34;&#34;
    with parallel_backend(&#39;loky&#39;):
        self.kernel.fit(x, y)
    score = self.kernel.feature_importances_
    self.scores = pd.Series(score, index=x.columns,
                            name=self.name).sort_values(ascending=False)
    return self.scores.copy()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></b></code>:
<ul class="hlist">
<li><code><a title="PineBioML.selection.SelectionPipeline.Choose" href="index.html#PineBioML.selection.SelectionPipeline.Choose">Choose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Diagnose" href="index.html#PineBioML.selection.SelectionPipeline.Diagnose">Diagnose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Plotting" href="index.html#PineBioML.selection.SelectionPipeline.Plotting">Plotting</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Report" href="index.html#PineBioML.selection.SelectionPipeline.Report">Report</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Select" href="index.html#PineBioML.selection.SelectionPipeline.Select">Select</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.fit" href="index.html#PineBioML.selection.SelectionPipeline.fit">fit</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.reference" href="index.html#PineBioML.selection.SelectionPipeline.reference">reference</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="PineBioML.selection.classification.SVM_selection"><code class="flex name class">
<span>class <span class="ident">SVM_selection</span></span>
<span>(</span><span>k)</span>
</code></dt>
<dd>
<div class="desc"><p>Using the support vector of linear support vector classifier as scoring method.
</p>
<p>SVM_selection is scale sensitive in result.
</p>
<p>&lt;<Feature Ranking Using Linear SVM>&gt; section 3.2</p>
<p>Initialize the selection pipeline.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>select top k important feature. k = -1 means selecting all, k = None means selecting the feature that have standarized score &gt; 1. Default = None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SVM_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using the support vector of linear support vector classifier as scoring method.    

    SVM_selection is scale sensitive in result.    

    &lt;&lt;Feature Ranking Using Linear SVM&gt;&gt; section 3.2

    &#34;&#34;&#34;

    def __init__(self, k):
        super().__init__(k=k)
        self.kernel = LinearSVC(dual=&#34;auto&#34;,
                                class_weight=&#34;balanced&#34;,
                                random_state=142)
        self.name = &#34;SVM&#34;

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html&#34;
        refer[
            &#34; publication - section 3.2&#34;] = &#34;https://www.csie.ntu.edu.tw/~cjlin/papers/causality.pdf&#34;
        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using the support vector of linear support vector classifier as scoring method.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        self.kernel.fit(x, y)
        svm_weights = np.abs(self.kernel.coef_).sum(axis=0)
        svm_weights /= svm_weights.sum()

        self.scores = pd.Series(svm_weights, index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.selection.classification.SVM_selection.Scoring"><code class="name flex">
<span>def <span class="ident">Scoring</span></span>(<span>self, x, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Using the support vector of linear support vector classifier as scoring method.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>a 2D array</code></dt>
<dd>The data to extract information.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>a 1D array</code></dt>
<dd>The target label for methods. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Series</code> or <code>pandas.DataFrame</code></dt>
<dd>The score for each feature. Some elements may be empty.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Scoring(self, x, y=None):
    &#34;&#34;&#34;
    Using the support vector of linear support vector classifier as scoring method.

    Args:
        x (pandas.DataFrame or a 2D array): The data to extract information.
        y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

    Returns:
        pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
    &#34;&#34;&#34;
    self.kernel.fit(x, y)
    svm_weights = np.abs(self.kernel.coef_).sum(axis=0)
    svm_weights /= svm_weights.sum()

    self.scores = pd.Series(svm_weights, index=x.columns,
                            name=self.name).sort_values(ascending=False)
    return self.scores.copy()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></b></code>:
<ul class="hlist">
<li><code><a title="PineBioML.selection.SelectionPipeline.Choose" href="index.html#PineBioML.selection.SelectionPipeline.Choose">Choose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Diagnose" href="index.html#PineBioML.selection.SelectionPipeline.Diagnose">Diagnose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Plotting" href="index.html#PineBioML.selection.SelectionPipeline.Plotting">Plotting</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Report" href="index.html#PineBioML.selection.SelectionPipeline.Report">Report</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Select" href="index.html#PineBioML.selection.SelectionPipeline.Select">Select</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.fit" href="index.html#PineBioML.selection.SelectionPipeline.fit">fit</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.reference" href="index.html#PineBioML.selection.SelectionPipeline.reference">reference</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="PineBioML.selection.classification.XGboost_selection"><code class="flex name class">
<span>class <span class="ident">XGboost_selection</span></span>
<span>(</span><span>k, unbalanced=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Using XGboost to scoring (gini impurity / entropy) features.</p>
<p>Warning: If data is too easy, boosting methods is difficult to give score to all features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>unbalanced</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>True to imply class weight to samples. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class XGboost_selection(SelectionPipeline):
    &#34;&#34;&#34;
    Using XGboost to scoring (gini impurity / entropy) features.

    Warning: If data is too easy, boosting methods is difficult to give score to all features.
    &#34;&#34;&#34;

    def __init__(self, k, unbalanced=True):
        &#34;&#34;&#34;
        Args:
            unbalanced (bool, optional): True to imply class weight to samples. Defaults to False.
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.unbalanced = unbalanced

        self.kernel = xgb.XGBClassifier(random_state=142, subsample=0.7)
        self.name = &#34;XGboost&#34;

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; document&#34;] = &#34;https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor.feature_importances_&#34;
        return refer

    def Scoring(self, x, y=None):
        &#34;&#34;&#34;
        Using XGboost to scoring (gini impurity / entropy) features.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

        Returns:
            pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
        &#34;&#34;&#34;
        if self.unbalanced:
            sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
        else:
            sample_weight = np.ones(len(y))

        y = OneHotEncoder(sparse_output=False).fit_transform(
            y.to_numpy().reshape(-1, 1))
        self.kernel.fit(x, y, sample_weight=sample_weight)
        score = self.kernel.feature_importances_
        self.scores = pd.Series(score, index=x.columns,
                                name=self.name).sort_values(ascending=False)
        return self.scores.copy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.selection.classification.XGboost_selection.Scoring"><code class="name flex">
<span>def <span class="ident">Scoring</span></span>(<span>self, x, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Using XGboost to scoring (gini impurity / entropy) features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>a 2D array</code></dt>
<dd>The data to extract information.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>a 1D array</code></dt>
<dd>The target label for methods. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Series</code> or <code>pandas.DataFrame</code></dt>
<dd>The score for each feature. Some elements may be empty.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Scoring(self, x, y=None):
    &#34;&#34;&#34;
    Using XGboost to scoring (gini impurity / entropy) features.

    Args:
        x (pandas.DataFrame or a 2D array): The data to extract information.
        y (pandas.Series or a 1D array): The target label for methods. Defaults to None.

    Returns:
        pandas.Series or pandas.DataFrame: The score for each feature. Some elements may be empty.
    &#34;&#34;&#34;
    if self.unbalanced:
        sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
    else:
        sample_weight = np.ones(len(y))

    y = OneHotEncoder(sparse_output=False).fit_transform(
        y.to_numpy().reshape(-1, 1))
    self.kernel.fit(x, y, sample_weight=sample_weight)
    score = self.kernel.feature_importances_
    self.scores = pd.Series(score, index=x.columns,
                            name=self.name).sort_values(ascending=False)
    return self.scores.copy()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></b></code>:
<ul class="hlist">
<li><code><a title="PineBioML.selection.SelectionPipeline.Choose" href="index.html#PineBioML.selection.SelectionPipeline.Choose">Choose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Diagnose" href="index.html#PineBioML.selection.SelectionPipeline.Diagnose">Diagnose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Plotting" href="index.html#PineBioML.selection.SelectionPipeline.Plotting">Plotting</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Report" href="index.html#PineBioML.selection.SelectionPipeline.Report">Report</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Select" href="index.html#PineBioML.selection.SelectionPipeline.Select">Select</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.fit" href="index.html#PineBioML.selection.SelectionPipeline.fit">fit</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.reference" href="index.html#PineBioML.selection.SelectionPipeline.reference">reference</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="PineBioML.selection.classification.essemble_selector"><code class="flex name class">
<span>class <span class="ident">essemble_selector</span></span>
<span>(</span><span>k=-1, RF_trees=1024, z_importance_threshold: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>A functional stack of diffirent methods.
<br>
What we do here is:
<br>
1. calculate feature importance in different methods.
<br>
2. standardize the scores and then averaging through methods.
<br>
3. If z_importance_threshold not None, then all features with averaging score higher than z_importance_threshold will be selected.
<br>
else top k feature with averaging score will be selected.
</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of features to be selected. Defaults to -1.</dd>
<dt><strong><code>RF_trees</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of trees using for randomforest. Defaults to 1024.</dd>
<dt><strong><code>z_importance_threshold</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The threshold to picking features. Defaults to None.</dd>
</dl>
<h2 id="todo">Todo</h2>
<p>auto adjust the RF_Trees by number of input features.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class essemble_selector(SelectionPipeline):
    &#34;&#34;&#34;
    A functional stack of diffirent methods.    
    What we do here is:    
        1. calculate feature importance in different methods.    
        2. standardize the scores and then averaging through methods.    
        3. If z_importance_threshold not None, then all features with averaging score higher than z_importance_threshold will be selected.    
           else top k feature with averaging score will be selected.    
    &#34;&#34;&#34;

    def __init__(self,
                 k=-1,
                 RF_trees=1024,
                 z_importance_threshold: int = None):
        &#34;&#34;&#34;

        Args:
            k (int, optional): The number of features to be selected. Defaults to -1.
            RF_trees (int, optional): number of trees using for randomforest. Defaults to 1024.
            z_importance_threshold (int, optional): The threshold to picking features. Defaults to None.
        
        Todo:
            auto adjust the RF_Trees by number of input features.
        &#34;&#34;&#34;
        self.k = k
        self.z_importance_threshold = z_importance_threshold

        self.kernels = {
            &#34;c45&#34;: DT_selection(k=k, strategy=&#34;c45&#34;),
            &#34;RF_gini&#34;: RF_selection(k=k, strategy=&#34;gini&#34;, trees=RF_trees),
            &#34;Lasso_Bisection&#34;: Lasso_bisection_selection(k=k),
            &#34;multi_Lasso&#34;: multi_Lasso_selection(k=k),
            &#34;SVM&#34;: SVM_selection(k=k),
            &#34;AdaBoost&#34;: AdaBoost_selection(k=k),
            &#34;XGboost&#34;: XGboost_selection(k=k),
            &#34;Lightgbm&#34;: Lightgbm_selection(k=k)
        }

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            self.name() +
            &#34; WARNING&#34;] = &#34;This method has no reference yet. That means the effectivity has not been proven yet. It somehow works in experience.&#34;
        return refer

    def Select(self, x, y):
        &#34;&#34;&#34;
        Calling all the methods in kernel sequancially.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.
            k (int): Number of feature to select. The result may less than k

        Returns:
            pandas.Series: The concatenated results. Top k (may less than k) important feature from diffient methods.
        &#34;&#34;&#34;
        results = []
        for method in self.kernels:
            print(&#34;Using &#34;, method, &#34; to select.&#34;)
            results.append(self.kernels[method].Select(x.copy(), y))
            print(method, &#34; is done.\n&#34;)

        name = pd.concat([pd.Series(i.index, name=i.name) for i in results],
                         axis=1)
        importance = pd.concat(results, axis=1)
        self.selected_score = importance
        return name, importance

    def fit(self, x, y):
        &#34;&#34;&#34;
        sklearn api

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods.
        &#34;&#34;&#34;
        name, importance = self.Select(x, y)

        return self

    def transform(self, x):
        z_scores = (self.selected_score - self.selected_score.mean()) / (
            self.selected_score.std() + 1e-4)
        z_scores = z_scores.mean(axis=1).sort_values(ascending=False)

        if self.z_importance_threshold is None:
            return x[z_scores.index[:self.k]]
        else:
            return x[z_scores[z_scores &gt; self.z_importance_threshold].index]

    def fit_transform(self, x, y):
        self.fit(x, y)
        return self.transform(x)

    def plotting(self):
        for method in self.kernels:
            self.kernels[method].plotting()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.selection.classification.essemble_selector.Select"><code class="name flex">
<span>def <span class="ident">Select</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Calling all the methods in kernel sequancially.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>a 2D array</code></dt>
<dd>The data to extract information.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>a 1D array</code></dt>
<dd>The target label for methods. Defaults to None.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of feature to select. The result may less than k</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Series</code></dt>
<dd>The concatenated results. Top k (may less than k) important feature from diffient methods.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Select(self, x, y):
    &#34;&#34;&#34;
    Calling all the methods in kernel sequancially.

    Args:
        x (pandas.DataFrame or a 2D array): The data to extract information.
        y (pandas.Series or a 1D array): The target label for methods. Defaults to None.
        k (int): Number of feature to select. The result may less than k

    Returns:
        pandas.Series: The concatenated results. Top k (may less than k) important feature from diffient methods.
    &#34;&#34;&#34;
    results = []
    for method in self.kernels:
        print(&#34;Using &#34;, method, &#34; to select.&#34;)
        results.append(self.kernels[method].Select(x.copy(), y))
        print(method, &#34; is done.\n&#34;)

    name = pd.concat([pd.Series(i.index, name=i.name) for i in results],
                     axis=1)
    importance = pd.concat(results, axis=1)
    self.selected_score = importance
    return name, importance</code></pre>
</details>
</dd>
<dt id="PineBioML.selection.classification.essemble_selector.fit_transform"><code class="name flex">
<span>def <span class="ident">fit_transform</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_transform(self, x, y):
    self.fit(x, y)
    return self.transform(x)</code></pre>
</details>
</dd>
<dt id="PineBioML.selection.classification.essemble_selector.plotting"><code class="name flex">
<span>def <span class="ident">plotting</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plotting(self):
    for method in self.kernels:
        self.kernels[method].plotting()</code></pre>
</details>
</dd>
<dt id="PineBioML.selection.classification.essemble_selector.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, x):
    z_scores = (self.selected_score - self.selected_score.mean()) / (
        self.selected_score.std() + 1e-4)
    z_scores = z_scores.mean(axis=1).sort_values(ascending=False)

    if self.z_importance_threshold is None:
        return x[z_scores.index[:self.k]]
    else:
        return x[z_scores[z_scores &gt; self.z_importance_threshold].index]</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></b></code>:
<ul class="hlist">
<li><code><a title="PineBioML.selection.SelectionPipeline.Choose" href="index.html#PineBioML.selection.SelectionPipeline.Choose">Choose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Diagnose" href="index.html#PineBioML.selection.SelectionPipeline.Diagnose">Diagnose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Plotting" href="index.html#PineBioML.selection.SelectionPipeline.Plotting">Plotting</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Report" href="index.html#PineBioML.selection.SelectionPipeline.Report">Report</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Scoring" href="index.html#PineBioML.selection.SelectionPipeline.Scoring">Scoring</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.fit" href="index.html#PineBioML.selection.SelectionPipeline.fit">fit</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.reference" href="index.html#PineBioML.selection.SelectionPipeline.reference">reference</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="PineBioML.selection.classification.multi_Lasso_selection"><code class="flex name class">
<span>class <span class="ident">multi_Lasso_selection</span></span>
<span>(</span><span>k, objective='regression')</span>
</code></dt>
<dd>
<div class="desc"><p>A stack of Lasso_bisection_selection. Because of collinearity, if there are a batch of featres with high corelation, only one of them will remain.
<br>
That leads to diffirent behavior between select k features in a time and select k//n features in n times.
</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>objective</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>one of {"Regression", "BinaryClassification"}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class multi_Lasso_selection(SelectionPipeline):
    &#34;&#34;&#34;
    A stack of Lasso_bisection_selection. Because of collinearity, if there are a batch of featres with high corelation, only one of them will remain.    
    That leads to diffirent behavior between select k features in a time and select k//n features in n times.    
    &#34;&#34;&#34;

    def __init__(self, k, objective=&#34;regression&#34;):
        &#34;&#34;&#34;
        Args:
            objective (str, optional): one of {&#34;Regression&#34;, &#34;BinaryClassification&#34;}
        &#34;&#34;&#34;
        super().__init__(k=k)
        self.name = &#34;multi_Lasso&#34;
        self.objective = objective

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refer = super().reference()
        refer[
            &#34; Warning&#34;] = &#34;We do not have a reference and this method&#39;s effectivity has not been proven yet.&#34;
        return refer

    def Select(self, x, y, n=5):
        &#34;&#34;&#34;
        Select k//n features for n times, and then concatenate the results.

        Args:
            x (pandas.DataFrame or a 2D array): The data to extract information.
            y (pandas.Series or a 1D array): The target label for methods. Defaults to None.
            n (int, optional): Number of batch which splits k to select. Defaults to 10.

        Returns:
            pandas.Series: The score for k selected features. May less than k.
            
        &#34;&#34;&#34;
        result = []
        if self.k == -1:
            self.k = x.shape[0]
        batch_size = self.k // n + 1

        for i in range(n):
            result.append(
                Lasso_bisection_selection(k=batch_size,
                                          objective=self.objective).Select(
                                              x, y))
            x = x.drop(result[-1].index, axis=1)
            if x.shape[1] == 0:
                break
        result = pd.concat(result)
        result = result - result.min()
        result.name = self.name

        self.selected_score = result.sort_values(ascending=False).head(self.k)
        return self.selected_score.copy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.selection.classification.multi_Lasso_selection.Select"><code class="name flex">
<span>def <span class="ident">Select</span></span>(<span>self, x, y, n=5)</span>
</code></dt>
<dd>
<div class="desc"><p>Select k//n features for n times, and then concatenate the results.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>a 2D array</code></dt>
<dd>The data to extract information.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>a 1D array</code></dt>
<dd>The target label for methods. Defaults to None.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of batch which splits k to select. Defaults to 10.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Series</code></dt>
<dd>The score for k selected features. May less than k.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Select(self, x, y, n=5):
    &#34;&#34;&#34;
    Select k//n features for n times, and then concatenate the results.

    Args:
        x (pandas.DataFrame or a 2D array): The data to extract information.
        y (pandas.Series or a 1D array): The target label for methods. Defaults to None.
        n (int, optional): Number of batch which splits k to select. Defaults to 10.

    Returns:
        pandas.Series: The score for k selected features. May less than k.
        
    &#34;&#34;&#34;
    result = []
    if self.k == -1:
        self.k = x.shape[0]
    batch_size = self.k // n + 1

    for i in range(n):
        result.append(
            Lasso_bisection_selection(k=batch_size,
                                      objective=self.objective).Select(
                                          x, y))
        x = x.drop(result[-1].index, axis=1)
        if x.shape[1] == 0:
            break
    result = pd.concat(result)
    result = result - result.min()
    result.name = self.name

    self.selected_score = result.sort_values(ascending=False).head(self.k)
    return self.selected_score.copy()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PineBioML.selection.SelectionPipeline" href="index.html#PineBioML.selection.SelectionPipeline">SelectionPipeline</a></b></code>:
<ul class="hlist">
<li><code><a title="PineBioML.selection.SelectionPipeline.Choose" href="index.html#PineBioML.selection.SelectionPipeline.Choose">Choose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Diagnose" href="index.html#PineBioML.selection.SelectionPipeline.Diagnose">Diagnose</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Plotting" href="index.html#PineBioML.selection.SelectionPipeline.Plotting">Plotting</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Report" href="index.html#PineBioML.selection.SelectionPipeline.Report">Report</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.Scoring" href="index.html#PineBioML.selection.SelectionPipeline.Scoring">Scoring</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.fit" href="index.html#PineBioML.selection.SelectionPipeline.fit">fit</a></code></li>
<li><code><a title="PineBioML.selection.SelectionPipeline.reference" href="index.html#PineBioML.selection.SelectionPipeline.reference">reference</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="PineBioML.selection" href="index.html">PineBioML.selection</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="PineBioML.selection.classification.AdaBoost_selection" href="#PineBioML.selection.classification.AdaBoost_selection">AdaBoost_selection</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.selection.classification.AdaBoost_selection.Scoring" href="#PineBioML.selection.classification.AdaBoost_selection.Scoring">Scoring</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PineBioML.selection.classification.DT_selection" href="#PineBioML.selection.classification.DT_selection">DT_selection</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.selection.classification.DT_selection.Scoring" href="#PineBioML.selection.classification.DT_selection.Scoring">Scoring</a></code></li>
<li><code><a title="PineBioML.selection.classification.DT_selection.entropy" href="#PineBioML.selection.classification.DT_selection.entropy">entropy</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PineBioML.selection.classification.Lasso_bisection_selection" href="#PineBioML.selection.classification.Lasso_bisection_selection">Lasso_bisection_selection</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.selection.classification.Lasso_bisection_selection.Select" href="#PineBioML.selection.classification.Lasso_bisection_selection.Select">Select</a></code></li>
<li><code><a title="PineBioML.selection.classification.Lasso_bisection_selection.coef_to_importance" href="#PineBioML.selection.classification.Lasso_bisection_selection.coef_to_importance">coef_to_importance</a></code></li>
<li><code><a title="PineBioML.selection.classification.Lasso_bisection_selection.create_kernel" href="#PineBioML.selection.classification.Lasso_bisection_selection.create_kernel">create_kernel</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PineBioML.selection.classification.Lasso_selection" href="#PineBioML.selection.classification.Lasso_selection">Lasso_selection</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.selection.classification.Lasso_selection.Scoring" href="#PineBioML.selection.classification.Lasso_selection.Scoring">Scoring</a></code></li>
<li><code><a title="PineBioML.selection.classification.Lasso_selection.create_kernel" href="#PineBioML.selection.classification.Lasso_selection.create_kernel">create_kernel</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PineBioML.selection.classification.Lightgbm_selection" href="#PineBioML.selection.classification.Lightgbm_selection">Lightgbm_selection</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.selection.classification.Lightgbm_selection.Scoring" href="#PineBioML.selection.classification.Lightgbm_selection.Scoring">Scoring</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PineBioML.selection.classification.RF_selection" href="#PineBioML.selection.classification.RF_selection">RF_selection</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.selection.classification.RF_selection.Scoring" href="#PineBioML.selection.classification.RF_selection.Scoring">Scoring</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PineBioML.selection.classification.SVM_selection" href="#PineBioML.selection.classification.SVM_selection">SVM_selection</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.selection.classification.SVM_selection.Scoring" href="#PineBioML.selection.classification.SVM_selection.Scoring">Scoring</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PineBioML.selection.classification.XGboost_selection" href="#PineBioML.selection.classification.XGboost_selection">XGboost_selection</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.selection.classification.XGboost_selection.Scoring" href="#PineBioML.selection.classification.XGboost_selection.Scoring">Scoring</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PineBioML.selection.classification.essemble_selector" href="#PineBioML.selection.classification.essemble_selector">essemble_selector</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.selection.classification.essemble_selector.Select" href="#PineBioML.selection.classification.essemble_selector.Select">Select</a></code></li>
<li><code><a title="PineBioML.selection.classification.essemble_selector.fit_transform" href="#PineBioML.selection.classification.essemble_selector.fit_transform">fit_transform</a></code></li>
<li><code><a title="PineBioML.selection.classification.essemble_selector.plotting" href="#PineBioML.selection.classification.essemble_selector.plotting">plotting</a></code></li>
<li><code><a title="PineBioML.selection.classification.essemble_selector.transform" href="#PineBioML.selection.classification.essemble_selector.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PineBioML.selection.classification.multi_Lasso_selection" href="#PineBioML.selection.classification.multi_Lasso_selection">multi_Lasso_selection</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.selection.classification.multi_Lasso_selection.Select" href="#PineBioML.selection.classification.multi_Lasso_selection.Select">Select</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>