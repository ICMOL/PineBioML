<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>PineBioML.model.supervised API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>PineBioML.model.supervised</code></h1>
</header>
<section id="section-intro">
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="PineBioML.model.supervised.Classification" href="Classification.html">PineBioML.model.supervised.Classification</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="PineBioML.model.supervised.Regression" href="Regression.html">PineBioML.model.supervised.Regression</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="PineBioML.model.supervised.Survival" href="Survival.html">PineBioML.model.supervised.Survival</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="PineBioML.model.supervised.Basic_tuner"><code class="flex name class">
<span>class <span class="ident">Basic_tuner</span></span>
<span>(</span><span>n_try: int,<br>n_cv: int,<br>target: str,<br>validate_penalty: bool,<br>TT_coef: float,<br>kernel_seed: int = None,<br>valid_seed: int = None,<br>optuna_seed: int = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Basic_tuner(ABC):
    &#34;&#34;&#34;
    The base class of tuner. A tuner is a wrapper of optuna + models    

    What the tuners do:    
        1. interface of optuna and models with sklearn api style    
        2. randomity management.    
        3. providing a uniform interface to regression and classiciation(binary and multi-class)    
    To conserve the reproducibility and to reduce the hyper parameter overfitting along the process of hyper parameter tuning,    
    we first using valid_seed to randomly initialize a tape of integers and it will sequentially be used in optuna trials.    
    &#34;&#34;&#34;

    def __init__(self,
                 n_try: int,
                 n_cv: int,
                 target: str,
                 validate_penalty: bool,
                 TT_coef: float,
                 kernel_seed: int = None,
                 valid_seed: int = None,
                 optuna_seed: int = None):
        &#34;&#34;&#34;

        Args:
            n_try (int): The number of trials optuna should try.
            n_cv (int): The number of folds to execute cross validation evaluation in iteration of optuna optimization.
            target (str): The target of optuna optimization. Notice that is different from the training loss of model.
            validate_penalty (bool): Deprecated.
            TT_coef (float): The power of penalty to overfitting by Tibshirani &amp; Tibshirani method. Ranges in [0, 1]. Add the difference between training score and cv score to the optimization target.
            kernel_seed (int, optional): Random seed for model. Defaults to None.
            valid_seed (int, optional): Random seed for cross validation. Defaults to None.
            optuna_seed (int, optional): Random seed for optuna&#39;s hyperparameter sampling. Defaults to None.

        ToDo:
            1. transfer the initialization of seed tape from __init__ to fit.    
            2. optuna pruner: See the section Acticating Pruners in https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html
            3. Winner&#39;s curse

        &#34;&#34;&#34;
        self.y_mapping = LabelEncoder()

        if validate_penalty == True:
            warnings.warn(
                &#34;validate_penalty will be remove in future. Use argument TT_coef.&#34;,
                DeprecationWarning,
                stacklevel=2)
            self.TT_coef = TT_coef
        else:
            self.TT_coef = 0

        self.n_cv = n_cv
        self.optuna_early_stop_counter = n_cv // 10 + 2
        self.n_try = n_try
        self.n_sample = 1
        self.n_opt_jobs = 1
        self.default = False
        self.training = True

        # initialize the random seeds
        if kernel_seed is None:
            self.kernel_seed = randint(16384)
        else:
            self.kernel_seed = kernel_seed

        if valid_seed is None:
            self.valid_seed = randint(16384)
        else:
            self.valid_seed = valid_seed

        if optuna_seed is None:
            self.optuna_seed = randint(16384)
        else:
            self.optuna_seed = optuna_seed

        # The random seed tapes for cross validation along the optuna&#39;s optimization trials.
        self.valid_seed_tape = RandomState(self.valid_seed).randint(low=0,
                                                                    high=16384,
                                                                    size=n_try)
        self.kernel_seed_tape = RandomState(self.kernel_seed).randint(
            low=0, high=16384, size=n_try)

        # Get the scorer
        self.metric = self.get_scorer(target)

        # the model optuna tuned
        self.optuna_model = None
        # the model with default params
        self.default_model = None
        # the better one of self.default_model or self.optuna_model
        self.best_model = None
        # the thresholds for binary classification along optuna tuning
        #self.thresholds = zeros(self.n_try) + 0.5
        # the threshold coresponding to the best optuna trial
        #self.best_threshold = 0.5
        # the stopping point of early stopping for Boosting methods.
        self.stop_points = zeros(self.n_try, dtype=int)

        self.explainer = None

    def reference(self) -&gt; dict[str, str]:
        &#34;&#34;&#34;
        This function will return reference of this method in python dict.    
        If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

        Returns:
            dict[str, str]: a dict of reference.
        &#34;&#34;&#34;
        refers = {
            &#34;optuna publication&#34;:
            &#34;https://dl.acm.org/doi/10.1145/3292500.3330701&#34;,
            &#34;optuna document&#34;: &#34;https://optuna.org/&#34;,
            &#34;sklearn publication&#34;:
            &#34;https://dl.acm.org/doi/10.5555/1953048.2078195&#34;
        }

        return refers

    @abstractmethod
    def name(self) -&gt; str:
        &#34;&#34;&#34;
        To be determined.

        Returns:
            str: Name of this tuner.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def is_regression(self) -&gt; bool:
        &#34;&#34;&#34;
        Returns:
            bool: True if the task of tuner is a regression task.
        &#34;&#34;&#34;
        pass

    def using_earlystopping(self) -&gt; bool:
        &#34;&#34;&#34;
        Returns:
            bool: True if applying earlystopping in optimizing training.
        &#34;&#34;&#34;
        return False

    def accepting_categorical_features(self) -&gt; bool:
        &#34;&#34;&#34;
        Returns:
            bool: Ture if kernel receives categorical (discreate) features.
        &#34;&#34;&#34;
        return False

    @abstractmethod
    def parms_range(self) -&gt; dict:
        &#34;&#34;&#34;
        model hyper-parameter search range.

        Returns:
            dict: {parameter_name : (parameter_name, parameter_dtype, lower_bound, upper_bound)}
        &#34;&#34;&#34;
        pass

    def parms_range_sparser(self, trial, search_setting):
        parameter_name, parameter_dtype, lower_bound, upper_bound = search_setting
        param = None

        if parameter_dtype in [&#34;float&#34;, &#34;int&#34;]:
            log = lower_bound &gt; 0 and upper_bound / lower_bound &gt; 10
            if parameter_dtype == &#34;float&#34;:
                param = trial.suggest_float(parameter_name,
                                            lower_bound,
                                            upper_bound,
                                            log=log)
            elif parameter_dtype == &#34;int&#34;:
                param = trial.suggest_int(parameter_name,
                                          lower_bound,
                                          upper_bound,
                                          log=log)
        else:
            if parameter_dtype == &#34;bool&#34;:
                param = trial.suggest_categorical(parameter_name,
                                                  [lower_bound, upper_bound])
            elif parameter_dtype == &#34;category&#34;:
                param = trial.suggest_categorical(parameter_name, lower_bound)
        if param is None:
            raise ValueError(
                &#34;parameter type not support, receaive parameter_dtype {}, parameter_name {}. Only one of [\&#34;float, int, bool, category] is supported.&#34;
                .format(parameter_dtype, parameter_name))
        return param

    @abstractmethod
    def create_model(self, trial, default, training) -&gt; BaseEstimator:
        &#34;&#34;&#34;
        Create model based on default setting or optuna trial over search range.

        Args:
            trial (optuna.trial.Trial): optuna trial in this call.
            default (bool): set True to use default hyper parameter
            training (bool): set True to when training.
            
        Returns :
            sklearn.base.BaseEstimator: A sklearn style model object.
        &#34;&#34;&#34;
        pass

    def optimize_fit(self, clr, train_data, sample_weight, valid_data):
        &#34;&#34;&#34;
        optimize_fit the polymorphism middle layer between model fitting and optuna optimize evaluate.    
        Specifically, optimize_fit is used for XGBoost/lightGBM/Catboost early stopping which requires validation data in .fit .
        However sklearn estimators such as randomforest does not provide such api.    

        Args:
            clr (classifier): classifier to fit.
            train_data (tuple): (train_x, train_y) .
            sample_weight (list-like): training sample weight.
            valid_data (tuple): (valid_x, valid_y) .

        Returns:
            sklearn.base.BaseEstimator: A fitted sklearn style model object.
        &#34;&#34;&#34;
        train_x, train_y = train_data
        valid_x, valid_y = valid_data

        return clr.fit(train_x, train_y, sample_weight=sample_weight)

    def clr_best_iteration(self, classifier) -&gt; int:
        &#34;&#34;&#34;
        the polymorphism to call classifier&#39;s early stopping rounds/iteration/point.    
        only need to be overide when subclass is implemented with early stopping.

        Args:
            classifier: the classifier object.

        Returns:
            int: the stopping point/rounds/iteration.
        &#34;&#34;&#34;
        pass

    def evaluate(self, trial, default=None, training=None) -&gt; float:
        &#34;&#34;&#34;
        To evaluate the score of this trial. you should call create_model instead of creating model manually in this function.
        
        Args:
            trial (optuna.trial.Trial): optuna trial in this call.
            default (bool): To use default hyper parameter. This argument will be passed to creat_model .    
            training (bool): whether it is under optuna optimization or not. False then evalutate will not tune a threshold or applying early stop. 
        Returns :
            float: The score. Decided by optimization target.
        &#34;&#34;&#34;
        if default is None:
            default = self.default
        if training is None:
            training = self.training

        # create the model using from this trial
        classifier_obj = self.create_model(trial, default, training=True)

        # create cross validation
        if self.is_regression():
            cv = KFold(n_splits=self.n_cv,
                       shuffle=True,
                       random_state=self.valid_seed_tape[trial.number])
        else:
            cv = StratifiedKFold(
                n_splits=self.n_cv,
                shuffle=True,
                random_state=self.valid_seed_tape[trial.number])

        # do cv
        score = zeros(self.n_cv)
        # thresholds not None only if  self.is_binary and not self.is_regression
        cv_thresholds = zeros(self.n_cv) + 0.5
        cv_stoppoint = zeros(self.n_cv)
        for i, (train_ind, test_ind) in enumerate(cv.split(self.x, self.y)):
            # train test split
            x_train = self.x.iloc[train_ind]
            y_train = self.y.iloc[train_ind]
            x_test = self.x.iloc[test_ind]
            y_test = self.y.iloc[test_ind]

            # sample_weight for imbalanced class.
            if self.is_regression():
                sample_weight = None
            else:
                sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;,
                                                      y=y_train)

            # fit the model on training fold
            fitted_clr = self.optimize_fit(clr=classifier_obj,
                                           train_data=(x_train, y_train),
                                           sample_weight=sample_weight,
                                           valid_data=(x_test, y_test))

            # record the stopping point of early stopping.
            if self.using_earlystopping() and training:
                cv_stoppoint[i] = self.clr_best_iteration(fitted_clr)

            # tune a threshold via roc for Binary classification
            &#34;&#34;&#34;
            if self.is_binary and not self.is_regression():
                if training:
                    fpr, tpr, thr = metrics.roc_curve(
                        y_test,
                        fitted_clr.predict_proba(x_test)[:, 1])
                    ### TODO: flexible threshold picker for various metrics.
                    cv_thresholds[i] = thr[abs(tpr - fpr).argmax()]
                else:
                    cv_thresholds[i] = self.thresholds[trial.number]
                # wrap the classifier before calculate scores
                if not default:
                    fitted_clr = Binary_threshold_wrapper(
                        fitted_clr, cv_thresholds[i])
                else:
                    fitted_clr = Binary_threshold_wrapper(fitted_clr, 0.5)
            &#34;&#34;&#34;

            # evaluate on testing fold
            test_score = self.metric(fitted_clr, x_test, y_test)
            train_score = self.metric(fitted_clr, x_train, y_train)

            score[i] = test_score + self.TT_coef * (test_score - train_score)

        # averaging over cv
        if training:
            #self.thresholds[trial.number] = cv_thresholds.sum() / self.n_cv
            self.stop_points[trial.number] = round(cv_stoppoint.sum() /
                                                   (self.n_cv - 1))
        return score.mean()

    def tune(self, x, y) -&gt; None:
        &#34;&#34;&#34;
        this function tunes the hyperparameters of a given kernel.
        
        Args:
            x (pandas.DataFrame or 2D-array): feature to extract information from.
            y (pandas.Series or 1D-array): The property we interested in.

        &#34;&#34;&#34;
        # input data
        self.x = x
        self.y = y

        # the total number of samples
        self.n_sample = x.shape[0]

        # check task
        self.check_task()

        # Make the sampler behave in a deterministic way.
        sampler = TPESampler(seed=self.optuna_seed,
                             multivariate=True,
                             n_startup_trials=self.n_try // 3)
        self.study = optuna.create_study(direction=&#34;maximize&#34;, sampler=sampler)

        print(
            &#34;optuna seed {self.optuna_seed}  |  validation seed {self.valid_seed}  |  model seed {self.kernel_seed}&#34;
            .format(self=self))
        print(&#34;    {} start tuning. it will take a while.&#34;.format(self.name()))
        # using optuna tuning hyper parameter
        self.training = True
        self.default = False
        self.study.optimize(self.evaluate,
                            n_trials=self.n_try,
                            show_progress_bar=False,
                            callbacks=[self.early_stop_callback],
                            n_jobs=self.n_opt_jobs)
        self.optuna_model = self.create_model(self.study.best_trial,
                                              default=False,
                                              training=False)

        # using default hyper parameter
        # the best_trial here is only a placeholder. It&#39;s not functional.
        self.default_performance = self.evaluate(self.study.best_trial,
                                                 default=True,
                                                 training=False)
        self.default_model = self.create_model(self.study.best_trial,
                                               default=True,
                                               training=False)
        #print(&#34;    default performance: {:.3f}  |  best performance: {:.3f}&#34;.
        #      format(self.default_performance, self.study.best_trial.value))
        if self.default_performance &gt; self.study.best_trial.value:
            # default better
            print(&#34;    default is better.&#34;)
            self.best_model = self.default_model
        else:
            # optuna better
            print(&#34;    optuna is better, best trial: &#34;,
                  self.study.best_trial.number)
            self.best_model = self.optuna_model
            &#34;&#34;&#34;
            # threshold tuner for binary classification
            if self.is_binary and not self.is_regression():
                self.best_threshold = self.thresholds[
                    self.study.best_trial.number]
            &#34;&#34;&#34;

        # release the datas
        self.x = None
        self.y = None

    def fit(self, x, y, retune=True):
        &#34;&#34;&#34;
        The sklearn.base.BaseEstimator fit api.    
        Set retune to True to retune the model using the given x and y, else using the previously tuned model to fit on given x and y.

        Args:
            x (pandas.DataFrame or 2D-array): feature to extract information from.
            y (pandas.Series or 1D-array): ground true.
            retune (bool): True to retune the model using given x and y, else using the tuned model to fit on given x, y.
        &#34;&#34;&#34;
        self.label_name = y.name

        # label encoding
        if not self.is_regression():
            y = Series(self.y_mapping.fit_transform(y),
                       index=y.index,
                       name=y.name)

        # tune the model.
        if retune:
            self.tune(x, y)

        # fit the model.
        if self.is_regression():
            sample_weight = None
        else:
            sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
        self.best_model.fit(x, y, sample_weight=sample_weight)

        return self

    def predict(self, x):
        &#34;&#34;&#34;
        The sklearn.base.BaseEstimator predict api.

        Args:
            x (pandas.DataFrame or 2D-array): feature to extract information from.

        Returns:
            1D-array: prediction
        &#34;&#34;&#34;
        # using the model.
        &#34;&#34;&#34;
        if self.is_binary and not self.is_regression():
            y_pred = (self.best_model.predict_proba(x)[:, 1]
                      &gt; self.best_threshold).astype(int)
        else:
        &#34;&#34;&#34;
        y_pred = self.best_model.predict(x)

        # label decoding
        if not self.is_regression():
            y_pred = self.y_mapping.inverse_transform(y_pred)
        y_pred = Series(y_pred, index=x.index, name=self.label_name)

        return y_pred

    def get_scorer(self, scorer_name: str):
        &#34;&#34;&#34;
        A lazy function to call sklearn scorers.    

        Args:
            scorer_name (str): abbreviation or formal name of sklearn scorers.

        Returns:
            scorer: A callable object that returns score.
        &#34;&#34;&#34;

        ### polymorphism
        scorer_name = scorer_name.lower().replace(&#34;-&#34;, &#34;_&#34;).replace(&#34; &#34;, &#34;_&#34;)

        ### common abbreviation
        nicknames = {
            &#34;acc&#34;: &#34;accuracy&#34;,
            &#34;auc&#34;: &#34;roc_auc&#34;,
            &#34;f1_score&#34;: &#34;f1&#34;,
            &#34;macro_f1&#34;: &#34;f1_macro&#34;,
            &#34;mcc&#34;: &#34;matthews_corrcoef&#34;,
            &#34;log_loss&#34;: &#34;neg_log_loss&#34;,
            &#34;cross_entropy&#34;: &#34;neg_log_loss&#34;,
            &#34;ce&#34;: &#34;neg_log_loss&#34;,
            &#34;bce&#34;: &#34;neg_log_loss&#34;,
            &#34;mse&#34;: &#34;neg_mean_squared_error&#34;,
            &#34;mean_squared_error&#34;: &#34;neg_mean_squared_error&#34;,
            &#34;mae&#34;: &#34;neg_mean_absolute_error&#34;,
            &#34;mean_absolute_error&#34;: &#34;neg_mean_absolute_error&#34;,
            &#34;mape&#34;: &#34;neg_mean_absolute_percentage_error&#34;,
            &#34;mean_absolute_percentage_error&#34;:
            &#34;neg_mean_absolute_percentage_error&#34;,
            &#34;rmse&#34;: &#34;neg_root_mean_squared_error&#34;,
            &#34;root_mean_squared_error&#34;: &#34;neg_root_mean_squared_error&#34;,
            &#34;qwk&#34;: &#34;quadratic_weighted_kappa&#34;,
            &#34;kappa&#34;: &#34;cohen_kappa&#34;
        }
        if scorer_name in nicknames:
            scorer_name = nicknames[scorer_name]

        self.metric_name = scorer_name

        # get the scorer
        ### kappa
        if scorer_name == &#34;quadratic_weighted_kappa&#34;:
            return metrics.make_scorer(metrics.cohen_kappa_score,
                                       weights=&#34;quadratic&#34;,
                                       response_method=&#34;predict&#34;,
                                       greater_is_better=True)
        elif scorer_name == &#34;cohen_kappa&#34;:
            return metrics.make_scorer(metrics.cohen_kappa_score,
                                       weights=None,
                                       response_method=&#34;predict&#34;,
                                       greater_is_better=True)
        ### others
        return metrics.get_scorer(scorer_name)

    def check_task(self) -&gt; bool:
        &#34;&#34;&#34;
        Uh....  this functions will do    
            1. some check for mis-using of method and types.    
            2. sparsing the task and target.    

        It will be nasty and full of dirty code. Make yourself at home.    
        

        Raises:
            ValueError: Check auc score only be used in binary classification

        Returns:
            bool: _description_
        &#34;&#34;&#34;
        # Check auc score only be used in binary classification
        is_auc = self.metric_name == &#34;roc_auc&#34;
        self.is_binary = len(self.y.value_counts()) == 2
        if is_auc and not self.is_binary:
            # roc_auc only can be used on binary classification. Do not try ovr, ovo. forget them.
            raise ValueError(
                &#34;auc only support binary classification, but more than 2 values are detected in y. Try &#39;f1_macro&#39;&#34;
            )

        # Sparse the metric
        scorer_kargs = {}
        for arguments in self.metric.__str__()[12:-1].split(&#34;, &#34;)[1:]:
            if &#34;(&#34; in arguments:  # for roc auc scorer: response_method=(&#39;decision_function&#39;, &#39;predict_proba&#39;)
                scorer_kargs[&#34;response_method&#34;] = &#34;predict_proba&#34;
            else:
                arg = arguments.split(&#34;=&#34;)
                if len(arg) == 2:
                    scorer_kargs[arg[0]] = arg[1]

        ### Response method
        self.metric_using_proba = scorer_kargs[&#34;response_method&#34;].find(
            &#34;_proba&#34;) != -1
        scorer_kargs.pop(&#34;response_method&#34;)

        ### Greater is better
        if &#39;greater_is_better&#39; in scorer_kargs:
            self.metric_great_better = not scorer_kargs[
                &#34;greater_is_better&#34;] == &#34;False&#34;
            scorer_kargs.pop(&#39;greater_is_better&#39;)
        else:
            self.metric_great_better = True
        self.metric_limit = 1 if self.metric_great_better else 0

        ### pos_label for f1 socres
        if &#34;pos_label&#34; in scorer_kargs:
            scorer_kargs.pop(&#34;pos_label&#34;)

        self.scorer_kargs = scorer_kargs

        return True

    def plot(self):
        &#34;&#34;&#34;
        Plot the learning process.
        &#34;&#34;&#34;
        from plotly import io
        fig = optuna.visualization.plot_optimization_history(
            self.study, target_name=self.metric_name)
        fig.add_hline(y=self.default_performance,
                      line_dash=&#34;dot&#34;,
                      annotation_text=&#34;Default setting&#34;,
                      annotation_position=&#34;bottom right&#34;)
        io.show(fig)

    def _explainer(self, x: DataFrame) -&gt; Explainer:
        print(&#34;Not implement&#34;)
        return None

    def shap_explain(self, x: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;

        Args:
            x (DataFrame): the data to be explained which has shape (Samples, Features).
        
        Returns:
            explainer (shap.Explaination): Indexing by (Features, Output, Samples)
        &#34;&#34;&#34;
        self.explainer = self._explainer(x)
        return self.explainer(x)

    def early_stop_callback(self, study, trial):
        if trial.value &gt;= self.metric_limit:
            self.optuna_early_stop_counter -= 1
            if self.optuna_early_stop_counter &lt;= 0:
                print(&#34;optuna evaluate value {} reachs {}&#39;s maximun value {}&#34;.
                      format(study.best_value, self.metric_name,
                             self.metric_limit))
                study.stop()

    def detail(self):
        &#34;&#34;&#34;
        show the experiment settings including:    
            1. models parameters searching range.    

        Returns:
            pandas.DataFrame
        &#34;&#34;&#34;
        parms_range = DataFrame(self.parms_range()).drop(0)
        if self.best_model is not None:
            best_params = self.best_model.get_params()

            parms_range.loc[4] = Series(
                [best_params[param] for param in parms_range.columns],
                index=parms_range.columns)

        parms_range = parms_range.T.reset_index()
        parms_range.columns = [
            &#34;parameter&#34;, &#34;dtype&#34;, &#34;lower_bound&#34;, &#34;upper_bound&#34;, &#34;result&#34;
        ]

        parms_range.index = [&#34; &#34;] * parms_range.shape[0]

        name_holder = DataFrame({
            self.name(): {
                &#34;parameter&#34;: None,
                &#34;dtype&#34;: None,
                &#34;lower_bound&#34;: None,
                &#34;upper_bound&#34;: None,
                &#34;result&#34;: None
            }
        }).T
        return concat([name_holder, parms_range], axis=0)</code></pre>
</details>
<div class="desc"><p>The base class of tuner. A tuner is a wrapper of optuna + models
</p>
<p>What the tuners do:
<br>
1. interface of optuna and models with sklearn api style
<br>
2. randomity management.
<br>
3. providing a uniform interface to regression and classiciation(binary and multi-class)
<br>
To conserve the reproducibility and to reduce the hyper parameter overfitting along the process of hyper parameter tuning,
<br>
we first using valid_seed to randomly initialize a tape of integers and it will sequentially be used in optuna trials.
</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_try</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of trials optuna should try.</dd>
<dt><strong><code>n_cv</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of folds to execute cross validation evaluation in iteration of optuna optimization.</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>str</code></dt>
<dd>The target of optuna optimization. Notice that is different from the training loss of model.</dd>
<dt><strong><code>validate_penalty</code></strong> :&ensp;<code>bool</code></dt>
<dd>Deprecated.</dd>
<dt><strong><code>TT_coef</code></strong> :&ensp;<code>float</code></dt>
<dd>The power of penalty to overfitting by Tibshirani &amp; Tibshirani method. Ranges in [0, 1]. Add the difference between training score and cv score to the optimization target.</dd>
<dt><strong><code>kernel_seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Random seed for model. Defaults to None.</dd>
<dt><strong><code>valid_seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Random seed for cross validation. Defaults to None.</dd>
<dt><strong><code>optuna_seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Random seed for optuna's hyperparameter sampling. Defaults to None.</dd>
</dl>
<h2 id="todo">Todo</h2>
<ol>
<li>transfer the initialization of seed tape from <strong>init</strong> to fit.
</li>
<li>optuna pruner: See the section Acticating Pruners in <a href="https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html">https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html</a></li>
<li>Winner's curse</li>
</ol></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="PineBioML.model.supervised.Classification.Classification_tuner" href="Classification.html#PineBioML.model.supervised.Classification.Classification_tuner">Classification_tuner</a></li>
<li><a title="PineBioML.model.supervised.Regression.Regression_tuner" href="Regression.html#PineBioML.model.supervised.Regression.Regression_tuner">Regression_tuner</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PineBioML.model.supervised.Basic_tuner.accepting_categorical_features"><code class="name flex">
<span>def <span class="ident">accepting_categorical_features</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accepting_categorical_features(self) -&gt; bool:
    &#34;&#34;&#34;
    Returns:
        bool: Ture if kernel receives categorical (discreate) features.
    &#34;&#34;&#34;
    return False</code></pre>
</details>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>Ture if kernel receives categorical (discreate) features.</dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.check_task"><code class="name flex">
<span>def <span class="ident">check_task</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_task(self) -&gt; bool:
    &#34;&#34;&#34;
    Uh....  this functions will do    
        1. some check for mis-using of method and types.    
        2. sparsing the task and target.    

    It will be nasty and full of dirty code. Make yourself at home.    
    

    Raises:
        ValueError: Check auc score only be used in binary classification

    Returns:
        bool: _description_
    &#34;&#34;&#34;
    # Check auc score only be used in binary classification
    is_auc = self.metric_name == &#34;roc_auc&#34;
    self.is_binary = len(self.y.value_counts()) == 2
    if is_auc and not self.is_binary:
        # roc_auc only can be used on binary classification. Do not try ovr, ovo. forget them.
        raise ValueError(
            &#34;auc only support binary classification, but more than 2 values are detected in y. Try &#39;f1_macro&#39;&#34;
        )

    # Sparse the metric
    scorer_kargs = {}
    for arguments in self.metric.__str__()[12:-1].split(&#34;, &#34;)[1:]:
        if &#34;(&#34; in arguments:  # for roc auc scorer: response_method=(&#39;decision_function&#39;, &#39;predict_proba&#39;)
            scorer_kargs[&#34;response_method&#34;] = &#34;predict_proba&#34;
        else:
            arg = arguments.split(&#34;=&#34;)
            if len(arg) == 2:
                scorer_kargs[arg[0]] = arg[1]

    ### Response method
    self.metric_using_proba = scorer_kargs[&#34;response_method&#34;].find(
        &#34;_proba&#34;) != -1
    scorer_kargs.pop(&#34;response_method&#34;)

    ### Greater is better
    if &#39;greater_is_better&#39; in scorer_kargs:
        self.metric_great_better = not scorer_kargs[
            &#34;greater_is_better&#34;] == &#34;False&#34;
        scorer_kargs.pop(&#39;greater_is_better&#39;)
    else:
        self.metric_great_better = True
    self.metric_limit = 1 if self.metric_great_better else 0

    ### pos_label for f1 socres
    if &#34;pos_label&#34; in scorer_kargs:
        scorer_kargs.pop(&#34;pos_label&#34;)

    self.scorer_kargs = scorer_kargs

    return True</code></pre>
</details>
<div class="desc"><p>Uh....
this functions will do
<br>
1. some check for mis-using of method and types.
<br>
2. sparsing the task and target.
</p>
<p>It will be nasty and full of dirty code. Make yourself at home.
</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>Check auc score only be used in binary classification</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd><em>description</em></dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.clr_best_iteration"><code class="name flex">
<span>def <span class="ident">clr_best_iteration</span></span>(<span>self, classifier) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clr_best_iteration(self, classifier) -&gt; int:
    &#34;&#34;&#34;
    the polymorphism to call classifier&#39;s early stopping rounds/iteration/point.    
    only need to be overide when subclass is implemented with early stopping.

    Args:
        classifier: the classifier object.

    Returns:
        int: the stopping point/rounds/iteration.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
<div class="desc"><p>the polymorphism to call classifier's early stopping rounds/iteration/point.
<br>
only need to be overide when subclass is implemented with early stopping.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>classifier</code></strong></dt>
<dd>the classifier object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>the stopping point/rounds/iteration.</dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.create_model"><code class="name flex">
<span>def <span class="ident">create_model</span></span>(<span>self, trial, default, training) ‑> sklearn.base.BaseEstimator</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def create_model(self, trial, default, training) -&gt; BaseEstimator:
    &#34;&#34;&#34;
    Create model based on default setting or optuna trial over search range.

    Args:
        trial (optuna.trial.Trial): optuna trial in this call.
        default (bool): set True to use default hyper parameter
        training (bool): set True to when training.
        
    Returns :
        sklearn.base.BaseEstimator: A sklearn style model object.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
<div class="desc"><p>Create model based on default setting or optuna trial over search range.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trial</code></strong> :&ensp;<code>optuna.trial.Trial</code></dt>
<dd>optuna trial in this call.</dd>
<dt><strong><code>default</code></strong> :&ensp;<code>bool</code></dt>
<dd>set True to use default hyper parameter</dd>
<dt><strong><code>training</code></strong> :&ensp;<code>bool</code></dt>
<dd>set True to when training.</dd>
</dl>
<p>Returns :
sklearn.base.BaseEstimator: A sklearn style model object.</p></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.detail"><code class="name flex">
<span>def <span class="ident">detail</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detail(self):
    &#34;&#34;&#34;
    show the experiment settings including:    
        1. models parameters searching range.    

    Returns:
        pandas.DataFrame
    &#34;&#34;&#34;
    parms_range = DataFrame(self.parms_range()).drop(0)
    if self.best_model is not None:
        best_params = self.best_model.get_params()

        parms_range.loc[4] = Series(
            [best_params[param] for param in parms_range.columns],
            index=parms_range.columns)

    parms_range = parms_range.T.reset_index()
    parms_range.columns = [
        &#34;parameter&#34;, &#34;dtype&#34;, &#34;lower_bound&#34;, &#34;upper_bound&#34;, &#34;result&#34;
    ]

    parms_range.index = [&#34; &#34;] * parms_range.shape[0]

    name_holder = DataFrame({
        self.name(): {
            &#34;parameter&#34;: None,
            &#34;dtype&#34;: None,
            &#34;lower_bound&#34;: None,
            &#34;upper_bound&#34;: None,
            &#34;result&#34;: None
        }
    }).T
    return concat([name_holder, parms_range], axis=0)</code></pre>
</details>
<div class="desc"><p>show the experiment settings including:
<br>
1. models parameters searching range.
</p>
<h2 id="returns">Returns</h2>
<p>pandas.DataFrame</p></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.early_stop_callback"><code class="name flex">
<span>def <span class="ident">early_stop_callback</span></span>(<span>self, study, trial)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def early_stop_callback(self, study, trial):
    if trial.value &gt;= self.metric_limit:
        self.optuna_early_stop_counter -= 1
        if self.optuna_early_stop_counter &lt;= 0:
            print(&#34;optuna evaluate value {} reachs {}&#39;s maximun value {}&#34;.
                  format(study.best_value, self.metric_name,
                         self.metric_limit))
            study.stop()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, trial, default=None, training=None) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, trial, default=None, training=None) -&gt; float:
    &#34;&#34;&#34;
    To evaluate the score of this trial. you should call create_model instead of creating model manually in this function.
    
    Args:
        trial (optuna.trial.Trial): optuna trial in this call.
        default (bool): To use default hyper parameter. This argument will be passed to creat_model .    
        training (bool): whether it is under optuna optimization or not. False then evalutate will not tune a threshold or applying early stop. 
    Returns :
        float: The score. Decided by optimization target.
    &#34;&#34;&#34;
    if default is None:
        default = self.default
    if training is None:
        training = self.training

    # create the model using from this trial
    classifier_obj = self.create_model(trial, default, training=True)

    # create cross validation
    if self.is_regression():
        cv = KFold(n_splits=self.n_cv,
                   shuffle=True,
                   random_state=self.valid_seed_tape[trial.number])
    else:
        cv = StratifiedKFold(
            n_splits=self.n_cv,
            shuffle=True,
            random_state=self.valid_seed_tape[trial.number])

    # do cv
    score = zeros(self.n_cv)
    # thresholds not None only if  self.is_binary and not self.is_regression
    cv_thresholds = zeros(self.n_cv) + 0.5
    cv_stoppoint = zeros(self.n_cv)
    for i, (train_ind, test_ind) in enumerate(cv.split(self.x, self.y)):
        # train test split
        x_train = self.x.iloc[train_ind]
        y_train = self.y.iloc[train_ind]
        x_test = self.x.iloc[test_ind]
        y_test = self.y.iloc[test_ind]

        # sample_weight for imbalanced class.
        if self.is_regression():
            sample_weight = None
        else:
            sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;,
                                                  y=y_train)

        # fit the model on training fold
        fitted_clr = self.optimize_fit(clr=classifier_obj,
                                       train_data=(x_train, y_train),
                                       sample_weight=sample_weight,
                                       valid_data=(x_test, y_test))

        # record the stopping point of early stopping.
        if self.using_earlystopping() and training:
            cv_stoppoint[i] = self.clr_best_iteration(fitted_clr)

        # tune a threshold via roc for Binary classification
        &#34;&#34;&#34;
        if self.is_binary and not self.is_regression():
            if training:
                fpr, tpr, thr = metrics.roc_curve(
                    y_test,
                    fitted_clr.predict_proba(x_test)[:, 1])
                ### TODO: flexible threshold picker for various metrics.
                cv_thresholds[i] = thr[abs(tpr - fpr).argmax()]
            else:
                cv_thresholds[i] = self.thresholds[trial.number]
            # wrap the classifier before calculate scores
            if not default:
                fitted_clr = Binary_threshold_wrapper(
                    fitted_clr, cv_thresholds[i])
            else:
                fitted_clr = Binary_threshold_wrapper(fitted_clr, 0.5)
        &#34;&#34;&#34;

        # evaluate on testing fold
        test_score = self.metric(fitted_clr, x_test, y_test)
        train_score = self.metric(fitted_clr, x_train, y_train)

        score[i] = test_score + self.TT_coef * (test_score - train_score)

    # averaging over cv
    if training:
        #self.thresholds[trial.number] = cv_thresholds.sum() / self.n_cv
        self.stop_points[trial.number] = round(cv_stoppoint.sum() /
                                               (self.n_cv - 1))
    return score.mean()</code></pre>
</details>
<div class="desc"><p>To evaluate the score of this trial. you should call create_model instead of creating model manually in this function.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trial</code></strong> :&ensp;<code>optuna.trial.Trial</code></dt>
<dd>optuna trial in this call.</dd>
<dt><strong><code>default</code></strong> :&ensp;<code>bool</code></dt>
<dd>To use default hyper parameter. This argument will be passed to creat_model .
</dd>
<dt><strong><code>training</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether it is under optuna optimization or not. False then evalutate will not tune a threshold or applying early stop. </dd>
</dl>
<p>Returns :
float: The score. Decided by optimization target.</p></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, x, y, retune=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, x, y, retune=True):
    &#34;&#34;&#34;
    The sklearn.base.BaseEstimator fit api.    
    Set retune to True to retune the model using the given x and y, else using the previously tuned model to fit on given x and y.

    Args:
        x (pandas.DataFrame or 2D-array): feature to extract information from.
        y (pandas.Series or 1D-array): ground true.
        retune (bool): True to retune the model using given x and y, else using the tuned model to fit on given x, y.
    &#34;&#34;&#34;
    self.label_name = y.name

    # label encoding
    if not self.is_regression():
        y = Series(self.y_mapping.fit_transform(y),
                   index=y.index,
                   name=y.name)

    # tune the model.
    if retune:
        self.tune(x, y)

    # fit the model.
    if self.is_regression():
        sample_weight = None
    else:
        sample_weight = compute_sample_weight(class_weight=&#34;balanced&#34;, y=y)
    self.best_model.fit(x, y, sample_weight=sample_weight)

    return self</code></pre>
</details>
<div class="desc"><p>The sklearn.base.BaseEstimator fit api.
<br>
Set retune to True to retune the model using the given x and y, else using the previously tuned model to fit on given x and y.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>2D-array</code></dt>
<dd>feature to extract information from.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>1D-array</code></dt>
<dd>ground true.</dd>
<dt><strong><code>retune</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to retune the model using given x and y, else using the tuned model to fit on given x, y.</dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.get_scorer"><code class="name flex">
<span>def <span class="ident">get_scorer</span></span>(<span>self, scorer_name: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_scorer(self, scorer_name: str):
    &#34;&#34;&#34;
    A lazy function to call sklearn scorers.    

    Args:
        scorer_name (str): abbreviation or formal name of sklearn scorers.

    Returns:
        scorer: A callable object that returns score.
    &#34;&#34;&#34;

    ### polymorphism
    scorer_name = scorer_name.lower().replace(&#34;-&#34;, &#34;_&#34;).replace(&#34; &#34;, &#34;_&#34;)

    ### common abbreviation
    nicknames = {
        &#34;acc&#34;: &#34;accuracy&#34;,
        &#34;auc&#34;: &#34;roc_auc&#34;,
        &#34;f1_score&#34;: &#34;f1&#34;,
        &#34;macro_f1&#34;: &#34;f1_macro&#34;,
        &#34;mcc&#34;: &#34;matthews_corrcoef&#34;,
        &#34;log_loss&#34;: &#34;neg_log_loss&#34;,
        &#34;cross_entropy&#34;: &#34;neg_log_loss&#34;,
        &#34;ce&#34;: &#34;neg_log_loss&#34;,
        &#34;bce&#34;: &#34;neg_log_loss&#34;,
        &#34;mse&#34;: &#34;neg_mean_squared_error&#34;,
        &#34;mean_squared_error&#34;: &#34;neg_mean_squared_error&#34;,
        &#34;mae&#34;: &#34;neg_mean_absolute_error&#34;,
        &#34;mean_absolute_error&#34;: &#34;neg_mean_absolute_error&#34;,
        &#34;mape&#34;: &#34;neg_mean_absolute_percentage_error&#34;,
        &#34;mean_absolute_percentage_error&#34;:
        &#34;neg_mean_absolute_percentage_error&#34;,
        &#34;rmse&#34;: &#34;neg_root_mean_squared_error&#34;,
        &#34;root_mean_squared_error&#34;: &#34;neg_root_mean_squared_error&#34;,
        &#34;qwk&#34;: &#34;quadratic_weighted_kappa&#34;,
        &#34;kappa&#34;: &#34;cohen_kappa&#34;
    }
    if scorer_name in nicknames:
        scorer_name = nicknames[scorer_name]

    self.metric_name = scorer_name

    # get the scorer
    ### kappa
    if scorer_name == &#34;quadratic_weighted_kappa&#34;:
        return metrics.make_scorer(metrics.cohen_kappa_score,
                                   weights=&#34;quadratic&#34;,
                                   response_method=&#34;predict&#34;,
                                   greater_is_better=True)
    elif scorer_name == &#34;cohen_kappa&#34;:
        return metrics.make_scorer(metrics.cohen_kappa_score,
                                   weights=None,
                                   response_method=&#34;predict&#34;,
                                   greater_is_better=True)
    ### others
    return metrics.get_scorer(scorer_name)</code></pre>
</details>
<div class="desc"><p>A lazy function to call sklearn scorers.
</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scorer_name</code></strong> :&ensp;<code>str</code></dt>
<dd>abbreviation or formal name of sklearn scorers.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>scorer</code></dt>
<dd>A callable object that returns score.</dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.is_regression"><code class="name flex">
<span>def <span class="ident">is_regression</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def is_regression(self) -&gt; bool:
    &#34;&#34;&#34;
    Returns:
        bool: True if the task of tuner is a regression task.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if the task of tuner is a regression task.</dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.name"><code class="name flex">
<span>def <span class="ident">name</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def name(self) -&gt; str:
    &#34;&#34;&#34;
    To be determined.

    Returns:
        str: Name of this tuner.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
<div class="desc"><p>To be determined.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Name of this tuner.</dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.optimize_fit"><code class="name flex">
<span>def <span class="ident">optimize_fit</span></span>(<span>self, clr, train_data, sample_weight, valid_data)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optimize_fit(self, clr, train_data, sample_weight, valid_data):
    &#34;&#34;&#34;
    optimize_fit the polymorphism middle layer between model fitting and optuna optimize evaluate.    
    Specifically, optimize_fit is used for XGBoost/lightGBM/Catboost early stopping which requires validation data in .fit .
    However sklearn estimators such as randomforest does not provide such api.    

    Args:
        clr (classifier): classifier to fit.
        train_data (tuple): (train_x, train_y) .
        sample_weight (list-like): training sample weight.
        valid_data (tuple): (valid_x, valid_y) .

    Returns:
        sklearn.base.BaseEstimator: A fitted sklearn style model object.
    &#34;&#34;&#34;
    train_x, train_y = train_data
    valid_x, valid_y = valid_data

    return clr.fit(train_x, train_y, sample_weight=sample_weight)</code></pre>
</details>
<div class="desc"><p>optimize_fit the polymorphism middle layer between model fitting and optuna optimize evaluate.
<br>
Specifically, optimize_fit is used for XGBoost/lightGBM/Catboost early stopping which requires validation data in .fit .
However sklearn estimators such as randomforest does not provide such api.
</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>clr</code></strong> :&ensp;<code>classifier</code></dt>
<dd>classifier to fit.</dd>
<dt><strong><code>train_data</code></strong> :&ensp;<code>tuple</code></dt>
<dd>(train_x, train_y) .</dd>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>list-like</code></dt>
<dd>training sample weight.</dd>
<dt><strong><code>valid_data</code></strong> :&ensp;<code>tuple</code></dt>
<dd>(valid_x, valid_y) .</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>sklearn.base.BaseEstimator</code></dt>
<dd>A fitted sklearn style model object.</dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.parms_range"><code class="name flex">
<span>def <span class="ident">parms_range</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def parms_range(self) -&gt; dict:
    &#34;&#34;&#34;
    model hyper-parameter search range.

    Returns:
        dict: {parameter_name : (parameter_name, parameter_dtype, lower_bound, upper_bound)}
    &#34;&#34;&#34;
    pass</code></pre>
</details>
<div class="desc"><p>model hyper-parameter search range.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>{parameter_name : (parameter_name, parameter_dtype, lower_bound, upper_bound)}</dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.parms_range_sparser"><code class="name flex">
<span>def <span class="ident">parms_range_sparser</span></span>(<span>self, trial, search_setting)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parms_range_sparser(self, trial, search_setting):
    parameter_name, parameter_dtype, lower_bound, upper_bound = search_setting
    param = None

    if parameter_dtype in [&#34;float&#34;, &#34;int&#34;]:
        log = lower_bound &gt; 0 and upper_bound / lower_bound &gt; 10
        if parameter_dtype == &#34;float&#34;:
            param = trial.suggest_float(parameter_name,
                                        lower_bound,
                                        upper_bound,
                                        log=log)
        elif parameter_dtype == &#34;int&#34;:
            param = trial.suggest_int(parameter_name,
                                      lower_bound,
                                      upper_bound,
                                      log=log)
    else:
        if parameter_dtype == &#34;bool&#34;:
            param = trial.suggest_categorical(parameter_name,
                                              [lower_bound, upper_bound])
        elif parameter_dtype == &#34;category&#34;:
            param = trial.suggest_categorical(parameter_name, lower_bound)
    if param is None:
        raise ValueError(
            &#34;parameter type not support, receaive parameter_dtype {}, parameter_name {}. Only one of [\&#34;float, int, bool, category] is supported.&#34;
            .format(parameter_dtype, parameter_name))
    return param</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self):
    &#34;&#34;&#34;
    Plot the learning process.
    &#34;&#34;&#34;
    from plotly import io
    fig = optuna.visualization.plot_optimization_history(
        self.study, target_name=self.metric_name)
    fig.add_hline(y=self.default_performance,
                  line_dash=&#34;dot&#34;,
                  annotation_text=&#34;Default setting&#34;,
                  annotation_position=&#34;bottom right&#34;)
    io.show(fig)</code></pre>
</details>
<div class="desc"><p>Plot the learning process.</p></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, x):
    &#34;&#34;&#34;
    The sklearn.base.BaseEstimator predict api.

    Args:
        x (pandas.DataFrame or 2D-array): feature to extract information from.

    Returns:
        1D-array: prediction
    &#34;&#34;&#34;
    # using the model.
    &#34;&#34;&#34;
    if self.is_binary and not self.is_regression():
        y_pred = (self.best_model.predict_proba(x)[:, 1]
                  &gt; self.best_threshold).astype(int)
    else:
    &#34;&#34;&#34;
    y_pred = self.best_model.predict(x)

    # label decoding
    if not self.is_regression():
        y_pred = self.y_mapping.inverse_transform(y_pred)
    y_pred = Series(y_pred, index=x.index, name=self.label_name)

    return y_pred</code></pre>
</details>
<div class="desc"><p>The sklearn.base.BaseEstimator predict api.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>2D-array</code></dt>
<dd>feature to extract information from.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>1D-array: prediction</p></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.reference"><code class="name flex">
<span>def <span class="ident">reference</span></span>(<span>self) ‑> dict[str, str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reference(self) -&gt; dict[str, str]:
    &#34;&#34;&#34;
    This function will return reference of this method in python dict.    
    If you want to access it in PineBioML api document, then click on the    &gt;Expand source code     

    Returns:
        dict[str, str]: a dict of reference.
    &#34;&#34;&#34;
    refers = {
        &#34;optuna publication&#34;:
        &#34;https://dl.acm.org/doi/10.1145/3292500.3330701&#34;,
        &#34;optuna document&#34;: &#34;https://optuna.org/&#34;,
        &#34;sklearn publication&#34;:
        &#34;https://dl.acm.org/doi/10.5555/1953048.2078195&#34;
    }

    return refers</code></pre>
</details>
<div class="desc"><p>This function will return reference of this method in python dict.
<br>
If you want to access it in PineBioML api document, then click on the
&gt;Expand source code
</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict[str, str]</code></dt>
<dd>a dict of reference.</dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.shap_explain"><code class="name flex">
<span>def <span class="ident">shap_explain</span></span>(<span>self, x: pandas.core.frame.DataFrame) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shap_explain(self, x: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;

    Args:
        x (DataFrame): the data to be explained which has shape (Samples, Features).
    
    Returns:
        explainer (shap.Explaination): Indexing by (Features, Output, Samples)
    &#34;&#34;&#34;
    self.explainer = self._explainer(x)
    return self.explainer(x)</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>the data to be explained which has shape (Samples, Features).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>explainer (shap.Explaination): Indexing by (Features, Output, Samples)</p></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.tune"><code class="name flex">
<span>def <span class="ident">tune</span></span>(<span>self, x, y) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tune(self, x, y) -&gt; None:
    &#34;&#34;&#34;
    this function tunes the hyperparameters of a given kernel.
    
    Args:
        x (pandas.DataFrame or 2D-array): feature to extract information from.
        y (pandas.Series or 1D-array): The property we interested in.

    &#34;&#34;&#34;
    # input data
    self.x = x
    self.y = y

    # the total number of samples
    self.n_sample = x.shape[0]

    # check task
    self.check_task()

    # Make the sampler behave in a deterministic way.
    sampler = TPESampler(seed=self.optuna_seed,
                         multivariate=True,
                         n_startup_trials=self.n_try // 3)
    self.study = optuna.create_study(direction=&#34;maximize&#34;, sampler=sampler)

    print(
        &#34;optuna seed {self.optuna_seed}  |  validation seed {self.valid_seed}  |  model seed {self.kernel_seed}&#34;
        .format(self=self))
    print(&#34;    {} start tuning. it will take a while.&#34;.format(self.name()))
    # using optuna tuning hyper parameter
    self.training = True
    self.default = False
    self.study.optimize(self.evaluate,
                        n_trials=self.n_try,
                        show_progress_bar=False,
                        callbacks=[self.early_stop_callback],
                        n_jobs=self.n_opt_jobs)
    self.optuna_model = self.create_model(self.study.best_trial,
                                          default=False,
                                          training=False)

    # using default hyper parameter
    # the best_trial here is only a placeholder. It&#39;s not functional.
    self.default_performance = self.evaluate(self.study.best_trial,
                                             default=True,
                                             training=False)
    self.default_model = self.create_model(self.study.best_trial,
                                           default=True,
                                           training=False)
    #print(&#34;    default performance: {:.3f}  |  best performance: {:.3f}&#34;.
    #      format(self.default_performance, self.study.best_trial.value))
    if self.default_performance &gt; self.study.best_trial.value:
        # default better
        print(&#34;    default is better.&#34;)
        self.best_model = self.default_model
    else:
        # optuna better
        print(&#34;    optuna is better, best trial: &#34;,
              self.study.best_trial.number)
        self.best_model = self.optuna_model
        &#34;&#34;&#34;
        # threshold tuner for binary classification
        if self.is_binary and not self.is_regression():
            self.best_threshold = self.thresholds[
                self.study.best_trial.number]
        &#34;&#34;&#34;

    # release the datas
    self.x = None
    self.y = None</code></pre>
</details>
<div class="desc"><p>this function tunes the hyperparameters of a given kernel.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>pandas.DataFrame</code> or <code>2D-array</code></dt>
<dd>feature to extract information from.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code> or <code>1D-array</code></dt>
<dd>The property we interested in.</dd>
</dl></div>
</dd>
<dt id="PineBioML.model.supervised.Basic_tuner.using_earlystopping"><code class="name flex">
<span>def <span class="ident">using_earlystopping</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def using_earlystopping(self) -&gt; bool:
    &#34;&#34;&#34;
    Returns:
        bool: True if applying earlystopping in optimizing training.
    &#34;&#34;&#34;
    return False</code></pre>
</details>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if applying earlystopping in optimizing training.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="PineBioML.model" href="../index.html">PineBioML.model</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="PineBioML.model.supervised.Classification" href="Classification.html">PineBioML.model.supervised.Classification</a></code></li>
<li><code><a title="PineBioML.model.supervised.Regression" href="Regression.html">PineBioML.model.supervised.Regression</a></code></li>
<li><code><a title="PineBioML.model.supervised.Survival" href="Survival.html">PineBioML.model.supervised.Survival</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="PineBioML.model.supervised.Basic_tuner" href="#PineBioML.model.supervised.Basic_tuner">Basic_tuner</a></code></h4>
<ul class="">
<li><code><a title="PineBioML.model.supervised.Basic_tuner.accepting_categorical_features" href="#PineBioML.model.supervised.Basic_tuner.accepting_categorical_features">accepting_categorical_features</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.check_task" href="#PineBioML.model.supervised.Basic_tuner.check_task">check_task</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.clr_best_iteration" href="#PineBioML.model.supervised.Basic_tuner.clr_best_iteration">clr_best_iteration</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.create_model" href="#PineBioML.model.supervised.Basic_tuner.create_model">create_model</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.detail" href="#PineBioML.model.supervised.Basic_tuner.detail">detail</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.early_stop_callback" href="#PineBioML.model.supervised.Basic_tuner.early_stop_callback">early_stop_callback</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.evaluate" href="#PineBioML.model.supervised.Basic_tuner.evaluate">evaluate</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.fit" href="#PineBioML.model.supervised.Basic_tuner.fit">fit</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.get_scorer" href="#PineBioML.model.supervised.Basic_tuner.get_scorer">get_scorer</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.is_regression" href="#PineBioML.model.supervised.Basic_tuner.is_regression">is_regression</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.name" href="#PineBioML.model.supervised.Basic_tuner.name">name</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.optimize_fit" href="#PineBioML.model.supervised.Basic_tuner.optimize_fit">optimize_fit</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.parms_range" href="#PineBioML.model.supervised.Basic_tuner.parms_range">parms_range</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.parms_range_sparser" href="#PineBioML.model.supervised.Basic_tuner.parms_range_sparser">parms_range_sparser</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.plot" href="#PineBioML.model.supervised.Basic_tuner.plot">plot</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.predict" href="#PineBioML.model.supervised.Basic_tuner.predict">predict</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.reference" href="#PineBioML.model.supervised.Basic_tuner.reference">reference</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.shap_explain" href="#PineBioML.model.supervised.Basic_tuner.shap_explain">shap_explain</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.tune" href="#PineBioML.model.supervised.Basic_tuner.tune">tune</a></code></li>
<li><code><a title="PineBioML.model.supervised.Basic_tuner.using_earlystopping" href="#PineBioML.model.supervised.Basic_tuner.using_earlystopping">using_earlystopping</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
